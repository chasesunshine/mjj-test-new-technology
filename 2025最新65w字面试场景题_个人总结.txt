# 2. 电商平台中订单未支付过期如何实现自动关单？
## 定时任务 (个人不建议)
优点：
    实现容易，成本低，基本不依赖其他组件。
缺点：
    时间可能不够精确。由于定时任务扫描的间隔是固定的，所以可能造成一些订单已经过期了一段时间才被扫描到，订单关闭的时间比正常时间晚一些。
    增加了数据库的压力。随着订单的数量越来越多，扫描的成本也会越来越大，执行时间也会被拉长，可能导致某些应该被关闭的订单迟迟没有被关闭。
总结：
    采用定时任务的方案比较适合对时间要求不是很敏感，并且数据量不太多的业务场景。
## JDK 延迟队列 DelayQueue
    DelayQueue 是 JDK 提供的一个无界队列，我们可以看到，DelayQueue 队列中的元素需要实现 Delayed，它只提供了一个方法，就是获取过期时间
    用户的订单生成以后，设置过期时间比如 30 分钟，放入定义好的 DelayQueue，然后创建一个线程，在线程中通过 while(true)不断的从 DelayQueue 中获取过期的数据。
    优点：
        不依赖任何第三方组件，连数据库也不需要了，实现起来也方便。
    缺点：
        因为 DelayQueue 是一个无界队列，如果放入的订单过多，会造成 JVMOOM。DelayQueue 基于 JVM 内存，如果 JVM 重启了，那所有数据就丢失了。
    总结：
        DelayQueue 适用于数据量较小，且丢失也不影响主业务的场景，比如内部系统的一些非重要通知，就算丢失，也不会有太大影响
个人解惑:
    while(true)不断的从 DelayQueue 中获取过期的数据。用的是@PostConstruct之类的方案做处理
## redis 过期监听
    redis 是一个高性能的 KV 数据库，除了用作缓存以外，其实还提供了过期监听的功能。在 redis.conf 中，配置 notify-keyspace-events Ex 即可开启此功能。
    然后在代码中继承 KeyspaceEventMessageListener，实现 onMessage 就可以监听过期的数据量。
    优点：
        由于redis的高性能，所以我们在设置key，或者消费key时，速度上是可以保证的。
    缺点：
        由于redis的key过期策略原因，当一个key过期时，redis无法保证立刻将其删除，自然我们的监听事件也无法第一时间消费到这个key，所以会存在一定的延迟。
        另外，在redis5.0之前，订阅发布中的消息并没有被持久化，自然也没有所谓的确认机制。所以一旦消费消息的过程中我们的客户端发生了宕机，这条消息就彻底丢失了。
    总结：
        redis 的过期订阅相比于其他方案没有太大的优势，在实际生产环境中，用得相对较少。
## Redisson 分布式延迟队列
    Redisson 是一个基于 redis 实现的 Java 驻内存数据网格，它不仅提供了一系列的分布式的 Java 常用对象，还提供了许多分布式服务。
    Redisson 除了提供我们常用的分布式锁外，还提供了一个分布式延迟队列RDelayedQueue，他是一种基于 zset 结构实现的延迟队列，其实现类是RedissonDelayedQueue。
    优点：
        使用简单，并且其实现类中大量使用 lua 脚本保证其原子性，不会有并发重复问题。
    缺点：
        需要依赖 redis（如果这算一种缺点的话）。
    总结：
        Redisson 是 redis 官方推荐的 JAVA 客户端，提供了很多常用的功能，使用简单、高效，推荐大家尝试使用
## RocketMQ 延迟消息
    延迟消息，当消息写入到 Broker 后，不会立刻被消费者消费，需要等待指定的时长后才可被消费处理的消息，称为延时消息。
    在订单创建之后，我们就可以把订单作为一条消息投递到 rocketmq，并将延迟时间设置为 30 分钟，这样，30 分钟后我们定义的 consumer 就可以消费到这条消息，然后检查用户是否支付了这个订单
    优点：
        可以使代码逻辑清晰，系统之间完全解耦，只需关注生产及消费消息即可。另外其吞吐量极高，最多可以支撑万亿级的数据量。
    缺点：
        相对来说 mq 是重量级的组件，引入 mq 之后，随之而来的消息丢失、幂等性问题等都加深了系统的复杂度。
    总结：
        通过 mq 进行系统业务解耦，以及对系统性能削峰填谷已经是当前高性能系统的标配。
## RabbitMQ 死信队列
    除了 RocketMQ 的延迟队列，RabbitMQ 的死信队列也可以实现消息延迟功能。当 RabbitMQ 中的一条正常消息，因为过了存活时间（TTL 过期）、队列长度超限、被消费者拒绝等原因无法被消费时，就会被当成一条死信消息，投递到死信队列。
    基于这样的机制，我们可以给消息设置一个 ttl，然后故意不消费消息，等消息过期就会进入死信队列，我们再消费死信队列即可。通过这样的方式，就可以达到同 RocketMQ 延迟消息一样的效果。
    优点：
        同 RocketMQ 一样，RabbitMQ 同样可以使业务解耦，基于其集群的扩展性，也可以实现高可用、高性能的目标。
    缺点：
        死信队列本质还是一个队列，队列都是先进先出，如果队头的消息过期时间比较长，就会导致后面过期的消息无法得到及时消费，造成消息阻塞。
    总结：
        除了增加系统复杂度之外，死信队列的阻塞问题也是需要我们重点关注的。

# 3. 如何设计一个秒杀系统
    在我看来，秒杀系统本质上就是一个满足大并发、高性能和高可用的分布式系统
    在我看来，秒杀其实主要解决两个问题，一个是并发读，一个是并发写。
## 3.3.4 处理热点数据
    处理热点数据通常有几种思路：一是优化，二是限制，三是隔离。
    先来说说优化。优化热点数据最有效的办法就是缓存热点数据，如果热点数据做了动静分离，那么可以长期缓存静态数据。但是，缓存热点数据更多的是“临时”缓存，即不管是静态数据还是动态数据，都用一个队列短暂地缓存数秒钟，由于队列长度有限，可以采用 LRU 淘汰算法替换
    再来说说限制。限制更多的是一种保护机制，限制的办法也有很多，例如对被访问商品的 ID 做一致性 Hash，然后根据 Hash 做分桶，每个分桶设置一个处理队列，这样可以把热点商品限制在一个请求队列里，防止因某些热点商品占用太多的服务器资源，而使其他请求始终得不到服务器的处理资源。
    最后介绍一下隔离。秒杀系统设计的第一个原则就是将这种热点数据隔离出来，不要让1%的请求影响到另外的 99%，隔离出来后也更方便对这 1%的请求做针对性的优化。具体到“秒杀”业务，我们可以在以下几个层次实现隔离。
        1. 业务隔离。把秒杀做成一种营销活动，卖家要参加秒杀这种营销活动需要单独报名，从技术上来说，卖家报名后对我们来说就有了已知热点，因此可以提前做好预热。
        2. 系统隔离。系统隔离更多的是运行时的隔离，可以通过分组部署的方式和另外 99%分开。秒杀可以申请单独的域名，目的也是让请求落到不同的集群中。
        3. 数据隔离。秒杀所调用的数据大部分都是热点数据，比如会启用单独的 Cache 集群或者 MySQL 数据库来放热点数据，目的也是不想 0.01%的数据有机会影响99.99%数据。

# 4. 如果你的系统的 QPS 突然提升 10 倍你会怎么设计？
## 4.1 硬件的扩展+微服务的拆分
## 4.2 高性能 RPC
    有小伙伴进行对比测试，Dubbo RPC 的性能，是 Feign RPC 的性能 10 倍。(这一点 mjj 本人提出质疑)
    mjj个人搜索解答：
        Dubbo RPC 在性能上优于 Feign RPC，主要原因包括以下几个方面：
        1. **协议与传输层优化**
           - **Dubbo** 默认采用 **Dubbo 协议**，基于 **TCP 长连接** 和 **Netty** 进行通信，减少了连接建立和断开的开销，适合高并发场景。
           - **Feign** 基于 **HTTP/1.1**（短连接），每次请求都需要建立和断开 TCP 连接，增加了网络开销，性能较低。
        2. **序列化方式**
           - **Dubbo** 支持 **Hessian2、Protobuf** 等高效的二进制序列化方式，数据体积小，解析速度快。
           - **Feign** 默认使用 **JSON/XML** 等文本格式，数据体积较大，解析效率较低。
        3. **通信模型**
           - **Dubbo** 支持 **异步调用** 和 **NIO（Netty）**，能更好地利用系统资源，提高吞吐量。
           - **Feign** 基于 **同步阻塞式 HTTP 调用**，在高并发场景下容易出现性能瓶颈。
        4. **负载均衡与容错机制**
           - **Dubbo** 内置 **多种负载均衡策略**（随机、轮询、最少活跃调用等），并支持 **服务端和客户端负载均衡**，减少网络延迟。
           - **Feign** 依赖 **Ribbon（或 Spring Cloud LoadBalancer）**，仅支持客户端负载均衡，策略相对较少。
        5. **服务治理优化**
           - **Dubbo** 提供 **更细粒度的流量控制**（如并发控制、超时管理），减少无效请求，提高整体性能。
           - **Feign** 的容错依赖 **Hystrix（已停止维护）** 或 **Sentinel**，相比 Dubbo 的 **Failover/Failfast 等策略**，性能优化空间较小。
        6. **网络层优化**
           - **Dubbo** 的 **RPC 协议** 属于 **传输层**，比 **Feign（应用层 HTTP）** 减少了协议解析的开销。
           - **Dubbo 支持多协议**（如 gRPC、RMI），可根据业务需求选择最优方案。
        **总结**
        Dubbo 在 **协议优化、序列化效率、通信模型、负载均衡、服务治理** 等方面均优于 Feign，因此在高并发、低延迟场景下性能更高。而 Feign 的优势在于 **易用性、RESTful 兼容性**，适合中小规模微服务架构。
## 4.3 消息队列消峰解耦
## 4.4 三级缓存架构
    以秒杀系统举例，活动预热商品信息可以提前缓存提供查询服务，库存数据可以提前缓存，下单流程可以完全走缓存扣减，秒杀结束后再异步写入数据库，数据库承担的压力就小的太多了。
## 4.5 数据库分库分表
    读写分离也就相当于数据库集群的方式降低了单节点的压力。而面对数据的急剧增长，原来的单库单表的存储方式已经无法支撑整个业务的发展，这时候就需要对数据库进行分库分表了。针对微服务而言垂直的分库本身已经是做过的，剩下大部分都是分表的方案了。
## 4.6 高可用
## 4.7 总结
   其实可以看到，怎么设计高并发系统这个问题本身他是不难的，无非是基于你知道的知识点，从物理硬件层面到软件的架构、代码层面的优化，使用什么中间件来不断提高系统的抗压能力。
   但是这个问题本身会带来更多的问题，微服务本身的拆分带来了分布式事务的问题，http、RPC 框架的使用带来了通信效率、路由、容错的问题，MQ 的引入带来了消息丢失、积压、事务消息、顺序消息的问题，
   缓存的引入又会带来一致性、雪崩、击穿的问题，数据库的读写分离、分库分表又会带来主从同步延迟、分布式 ID、事务一致性的问题，而为了解决这些问题我们又要不断的加入各种措施熔断、限流、降级、离线核对、预案处理等等来防止和追溯这些问题。

# 6. 如何从零搭建 10 万级 QPS 大流量、高并发优惠券系统
### 6.3.1 存储瓶颈及解决方案
    瓶颈：
        在系统架构中，我们使用了 MySQL、Redis 作为存储组件。我们知道，单个服务器的 I/O 能力终是有限的，在实际测试过程中，能够得到如下的数据：
         单个 MySQL 的每秒写入在 4000 QPS 左右，超过这个数字，MySQL 的I/O 时延会剧量增长。
         MySQL 单表记录到达了千万级别，查询效率会大大降低，如果过亿的话，数据查询会成为一个问题。
         Redis 单分片的写入瓶颈在 2w
    解决方案：
        1. 读写分离。在查询券模板、查询券记录等场景下，我们可以将 MySQL 进行读写分离，让这部分查询流量走 MySQL 的读库，从而减轻 MySQL 写库的查询压力。
        2. 分治。在软件设计中，有一种分治的思想，对于存储瓶颈的问题，业界常用的方案就是分而治之：流量分散、存储分散，即：分库分表。
        3. 发券，归根结底是要对用户的领券记录做持久化存储。对于 MySQL 本身 I/O瓶颈来说，我们可以在不同服务器上部署 MySQL 的不同分片，对 MySQL做水平扩容，这样一来，写请求就会分布在不同的 MySQL 主机上，这样就能够大幅提升 MySQL 整体的吞吐量。
        4. 给用户发了券，那么用户肯定需要查询自己获得的券。基于这个逻辑，我们以user_id 后四位为分片键，对用户领取的记录表做水平拆分，以支持用户维度的领券记录的查询。
        5. 每种券都有对应的数量，在给用户发券的过程中，我们是将发券数记录在Redis 中的，大流量的情况下，我们也需要对 Redis 做水平扩容，减轻 Redis单机的压力。

# 14. 如何避免超预期的高并发压力压垮系统？
    解决这种问题的一个主要手段就是限流，即拒绝部分访问请求，使访问负载压力降低到一个系统可以承受的程度。这样虽然有部分用户访问失败，但是整个系统依然是可用的，依然能对外提供服务，而不是因为负载压力太大而崩溃，导致所有用户都不能访问。为此，我们准备开发一个限流器，产品名称为“Diana”。
## 14.1 需求分析
    我们将 Diana 定位为一个限流器组件，即 Diana 的主要应用场景是部署在微服务网关或者其他 HTTP 服务器入口，以过滤器的方式对请求进行过滤，对超过限流规则的请求返回“服务不可用”HTTP 响应。
    Diana 的限流规则可通过配置文件获取，并需要支持本地配置和远程配置两种方式，远程配置优先于本地配置。限流方式包括：
         全局限流：针对所有请求进行限流，即保证整个系统处理的请求总数满足限流配置。
         账号限流：针对账号进行限流，即对单个账号发送的请求进行限流。
         设备限流：针对设备进行限流，即对单个客户端设备发送的请求进行限流。
         资源限流：针对某个资源（即某个 URL）进行限流，即保证访问该资源的请求总数满足限流配置。
    并且 Diana 设计应遵循开闭原则，能够支持灵活的限流规则功能扩展，即未来在不修改现有代码和兼容现有配置文件的情况下，支持新的配置规则。
### 14.2.2 高可用设计
   为了保证配置中心服务器和 Redis 服务器宕机时，限流器组件的高可用。限流器应具有自动降级功能，即配置中心不可用，则使用本地配置；Redis 服务器不可用，则降级为本地限流。
## 14.3 详细设计
    常用的限流算法有 4 种，
    固定窗口（Window）限流算法，滑动窗口（Sliding Window）限流算法，漏桶（Leaky Bucket）限流算法，令牌桶（Token Bucket）限流算法。

# 17. 如何让系统抗住双十一的预约抢购活动？
    我们先把需求梳理一下，总的来说，实现一个抢购系统大概可以分为四个阶段。
     商品预约：用户进入商品详情页面，获取购买资格，并等待商品抢购倒计时。
     等待抢购：等待商品抢购倒计时，直到商品开放抢购。
     商品抢购：商品抢购倒计时结束，用户提交抢购订单，排队等待抢购结果，抢购成功后，扣减系统库存，生成抢购订单。
     订单支付：等待用户支付成功后，系统更新订单状态，通知用户购买成功。
    接下来，我们就针对各阶段容易出现的问题，来分析其中的技术考点和解决方案
# 17.1 商品预约阶段
    那么问题来了：如何在高并发量的情况下，让每个用户都能得到抢购资格呢？这是预约抢购场景第一个技术考察点。 那你可以基于“06 | 分布式系统中，如何回答锁的实现原理？”来控制抢购资格的发放。
    我们基于 Redis 实现分布式锁（这是最常用的方式），在加锁的过程中，实际上是给Key 键设置一个值，为避免死锁，还要给
        SET lock_key unique_value NX PX 1000
     lock_key 就是 key 键；
     unique_value 是客户端生成的唯一的标识；
     NX 代表只在 lock_key 不存在时，才对 lock_key 进行设置操作；
     PX 10000 表示设置 lock_key 的过期时间为 10s，这是为了避免客户端发生异常而无法释放锁。
# 17.2 等待抢购阶段
    那么问题来了：如何解决等待抢购时间内的流量突增问题呢？有两个解决思路。
     页面静态化
     服务端限流
# 17.3 商品抢购阶段
    从“商品抢购阶段的架构设计”中我们可以总结出三个技术考点：流量削峰、扣减库存、分库分表。
# 17.4 订单支付阶段
    订单支付后操作（异常）
    所以你还要考虑“05 | 海量并发场景下，如何回答分布式事务一致性问题？”中，可靠消息投递机制：
    先做消息的本地存储，再通过异步重试机制，来实现消息的补偿。
    比如当支付平台回调订单系统，然后在更新状态的同时，插入一个消息，之后再返回第三方支付操作成功的结果。
    最后，通过数据库中的这条消息，再异步推送其他系统，完成后续的工作。
    mjj个人答疑解惑：(详见 SpringBoot订单支付后消息可靠补偿方案.md)
        SpringBoot订单支付后消息可靠补偿方案
        在订单支付后的操作中，发送异步消息到积分和短信服务时可能会遇到网络抖动、服务不可用等异常情况。下面提供一个完整的可靠消息补偿方案，包含本地消息表和异步重试机制。
            方案设计
            1. 本地消息表：在业务数据库中创建消息表，记录需要发送的消息
            2. 事务一致性：将业务操作和消息记录放在同一个事务中
            3. 定时任务：定期扫描未完成的消息进行重试
            4. 幂等处理：消费端需要实现幂等性，防止重复消费

# 18. 如果让你设计一个支持千万级流量的架构，你会怎么设计？
## 18.1 前端优化
    前端的优化主要有三个环节：减少请求次数、页面静态化、边缘计算。
        减少请求次数：减少前端脚本与后端服务的请求次数，有三种方案。
         增加缓存控制
         减少图像的请求次
         减少脚本的请求
## 18.2 后端优化
    后端环节的性能问题，可以从基础设施层面、网络层面、架构层面三个角度进行考量

# 19. 如何设计 RPC 框架在 10 万 QPS 下实现毫秒级的服务调用？
## 19.2 如何提升网络传输性能
    常见的五种 I/O 模型：
        同步阻塞 I/O；
        同步非阻塞 I/O；
        同步多路 I/O 复用；
        信号驱动 I/O；
        异步 I/O。
    你需要理解这五种 I/O 模型的区别和特点，不过在理解上你可能会有些难度，所以我来做个比喻方便你理解。
    我们把 I/O 过程比喻成烧水倒水的过程，等待资源（就是烧水的过程），使用资源（就是倒水的过程）：
        如果你站在灶台边上一直等着（等待资源）水烧开，然后倒水（使用资源），那么就是同步阻塞 I/O；
        如果你偷点儿懒，在烧水的时候躺在沙发上看会儿电视（不再时时刻刻等待资源），但是还是要时不时地去看看水开了没有，一旦水开了，马上去倒水（使用资源），那么这就是同步非阻塞 I/O；
        如果你想要洗澡，需要同时烧好多壶水，那你就在看电视的间隙去看看哪壶水开了（等待多个资源），哪一壶开了就先倒哪一壶，这样就加快了烧水的速度，这就是同步多路I/O 复用；
        不过你发现自己总是跑厨房去看水开了没，太累了，于是你考虑给你的水壶加一个报警器（信号），只要水开了就马上去倒水，这就是信号驱动 I/O；
        最后一种就高级了，你发明了一个智能水壶，在水烧好后自动就可以把水倒好，这就是异步 I/O。
    这五种 I/O 模型中最被广泛使用的是多路 I/O 复用，Linux 系统中的 select、epoll等系统调用都是支持多路 I/O 复用模型的，Java 中的高性能网络框架 Netty 默认也是使用这种模型。你可以选择它。
## 19.3 选择合适的序列化方式
    综合上面的几个考虑点，在我看来，我们的序列化备选方案主要有以下几种：
        首先是大家熟知的 JSON，它起源于 JavaScript 是一种最广泛使用的序列化协议，它的优势简单易用，同时在性能上相比 XML
        Thrift 是 Facebook 开源的高性能的序列化协议，也是一个轻量级的 RPC 框架；
        Protobuf 是谷歌开源的序列化协议。它们的共同特点是无论在空间上还是时间上都有着很高的性能，缺点就是由于 IDL 存在带来一些使用上的不方便
    那么你要如何选择这几种序列化协议呢？这里我给你几点建议：
    如果对于性能要求不高，在传输数据占用带宽不大的场景下可以使用 JSON 作为序列化协议；
    如果对于性能要求比较高，那么使用 Thrift 或者 Protobuf 都可以。而 Thrift 提供了配套的 RPC 框架，所以想要一体化的解决方案，你可以优先考虑 Thrift；
    在一些存储的场景下，比如说你的缓存中存储的数据占用空间较大，那么你可以考虑使用 Protobuf 替换 JSON

# 21. 如何根据应用场景选择合适的消息中间件？
    引入消息中间件的目的是让它来扛住海量流量，流量先进入到消息队列中，然后消费端下游系统可以慢慢消费消息中间件中的数据，这样能有效保护下游系统不被瞬时的流量击破。

    在进行具体的选型时，我们可以结合自己团队的实际情况。
     如果公司或团队的技术栈以 Golang 为主，建议选择 RabbitMQ，RabbitMQ 在性能上的缺陷可以通过搭建多套集群加以规避。
     如果公司或团队的技术栈以 Java 为主，我建议使用 Kafka 或 RocketMQ。RocketMQ 和 Kafka 都是性能优秀的中间件，在这两者之间进行选择时可以更多地关注功能特性。RocketMQ 提供了消息重试、消息过滤、消息轨迹、消息检索
        等功能特性，特别是 RocketMQ 的消息检索功能，因此 RocketMQ 很适合核心业务场景。而 kafka 更加擅长于日志、大数据计算、流式计算等场景。

# 22. 如何提升 RocketMQ 顺序消费性能
## 22.1 RocketMQ 顺序消费设计缺
    回顾上面 RocketMQ 实现顺序消费的核心关键词，我们发现其实就是加锁、加锁、加锁。没错，为了实现顺序消费，RocketMQ 需要进行三次加锁：
     进行队列负载平衡后，对新分配的队列，并不能立即进行消息拉取，必须先在Broker 端获取队列的锁；
     消费端在正式消费数据之前，需要锁定 MessageQueue 和 ProceeQueue。上述三把锁的控制，让并发度受到了队列数量的限制。在互联网、高并发编程领域，通常是“谈锁色变”，锁几乎成为了性能低下的代名词。试图减少锁的使用、缩小锁的范围几乎是性能优化的主要手段。

# 23. 使用分布式调度框架该考虑哪些问题？
    因为定时调度任务的调度频率直接决定了消息发送的实时性，随着需要调度的任务越来越多，大部分定时调度框架对秒级别的定时调度都不太友好。这时的调度通常都是分钟级的，但分钟级的调度会给任务带来较大的延迟，这是大部分业务无法容忍的，怎么办呢？

    ElasticJob 可以通过支持流式任务解决这个问题。具体的思路是：将任务配置为按照分钟级进行调度，例如每分钟执行一次调度。每次调度按照分页去查找数据，处理完一批数据，再查询下一批，如果查到待处理数据，就继续处理数据，直到没有待处理数据时，
    才结束本次业务处理。如果本次处理时间超过了一个调度周期，那么利用 ElasticJob 的任务错过补偿执行机制会再触发一次调度。

    在业务高峰期，这种方式基本上提供了准实时的处理效果。只有在业务量较少时，如果处理完一批数据后没有其他待处理的数据，这时新到的数据才会延迟 1 分钟执行。
    综合来看，通过支持流式任务，我们可以极大地提高数据的处理时效。

    具体实现看 个人工作总结 2025/05/13 - java ElasticJob支持流式任务

# 26. 如何解决高并发下的库存抢购超卖少买问题？
## 26.3 令牌库存
    具体是使用 Redis 中的 list 保存多张令牌来代表库存，一张令牌就是一个库存，用户抢库存时拿到令牌的用户可以继续支付
        1 //放入三个库存
        2 redis> lpush prod_1475_stock_queue_1 stock_1
        3 redis> lpush prod_1475_stock_queue_1 stock_2
        4 redis> lpush prod_1475_stock_queue_1 stock_3
        5
        6 //取出一个，超过 0.5 秒没有返回，那么抢库存失败
        7 redis> brpop prod_1475_stock_queue_1 0.5
    具体实现看 个人工作总结 2025/05/13 - 使用RedisTemplate的List实现库存令牌管理
## 26.4 多库存秒杀
## 26.5 自旋互斥超时锁
## 26.6 CAS 乐观锁：锁操作后置
    除此之外我再推荐一个实现方式：CAS 乐观锁。相对于自旋互斥锁来说，它在并发争抢库存线程少的时候效率会更好。通常，我们用锁的实现方式是先抢锁，然后，再对数据进行操作。这个方式需要先抢到锁才能继续，而抢锁是有性能损耗的，即使没有其他线程抢锁，这个消耗仍旧存在。
    CAS 乐观锁的核心实现为：记录或监控当前库存信息或版本号，对数据进行预操作。
## 26.7 Redis Lua 方式实现 Redis 锁
    与“事务+乐观锁”类似的实现方式还有一种，就是使用 Redis 的 Lua 脚本实现多步骤库存操作。因为 Lua 脚本内所有操作都是连续的，这个操作不会被其他操作打断，所以不存在锁争抢问题。

# 27. 为什么高并发下数据写入不推荐关系数据库？
    总结(deepseek)
    高并发下不推荐关系数据库的主要原因在于其索引结构、锁机制、ACID特性实现和主从架构等方面的设计限制了写入性能。B+Tree索引的随机IO、锁竞争、事务开销和单点写入等问题使得关系数据库难以应对极高并发的写入场景1310。
    对于电商供应链系统，没有放之四海而皆准的数据库解决方案。合理的策略是根据不同业务场景选择最适合的数据库技术，形成混合架构：
        核心事务：关系数据库（MySQL/PostgreSQL）+ Redis缓存
        商品信息：文档数据库（MongoDB）
        用户行为：搜索引擎（ElasticSearch）或时序数据库
        分析报表：列式存储（ClickHouse）或数据仓库
        推荐系统：图数据库（Neo4j）

# 31. 假设数据库成为了性能瓶颈点，动态数据查询如何提升效率
## 31.4 缓存分类
   在我们日常开发中，常见的缓存主要就是静态缓存、分布式缓存和热点本地缓存这三种。
       静态缓存在 Web 1.0 时期是非常著名的，它一般通过生成 Velocity 模板或者静态HTML 文件来实现静态缓存。
       分布式缓存的大名可谓是如雷贯耳了，我们平时耳熟能详的 Memcached、Redis 就是分布式缓存的典型例子。
       那么我们会在代码中使用一些本地缓存方案，如 HashMap，Guava Cache 或者是Ehcache 等，
## 31.5 缓存的不足
    首先，缓存比较适合于读多写少的业务场景，并且数据最好带有一定的热点属性。这是因为缓存毕竟会受限于存储介质不可能缓存所有数据，那么当数据有热点属性的时候才能保证一定的缓存命中率。
    其次，缓存会给整体系统带来复杂度，并且会有数据不一致的风险。当更新数据库成功，更新缓存失败的场景下，缓存中就会存在脏数据。
    再次，之前提到缓存通常使用内存作为存储介质，但是内存并不是无限的。因此，我们在使用缓存的时候要做数据存储量级的评估，对于可预见的需要消耗极大存储成本的数据，要慎用缓存方案。同时，缓存一定要设置过期时间，这样可以保证缓存中的会是热点数据。
    最后，缓存会给运维也带来一定的成本，运维需要对缓存组件有一定的了解，在排查问题的时候也多了一个组件需要考虑在内。

# 34. 线上的 API 接口响应比较慢，该如何快速排查和定位问题？
## 34.1 定位问题
### 34.1.1 慢查询日志
    通常情况下，为了定位 sql 的性能瓶颈，我们需要开启 mysql 的慢查询日志。把超过指定时间的 sql 语句，单独记录下来，方面以后分析和定位问题。
    开启慢查询日志需要重点关注三个参数：
         slow_query_log 慢查询开关
         slow_query_log_file 慢查询日志存放的路径
         long_query_time 超过多少秒才会记录日志
    通过 mysql 的 set 命令可以设置：
        1 set global slow_query_log='ON';
        2 set global slow_query_log_file='/usr/local/mysql/data/slow.log';
        3 set global long_query_time=2;
    设置完之后，如果某条 sql 的执行时间超过了 2 秒，会被自动记录到 slow.log 文件中。
    当然也可以直接修改配置文件 my.cnf
        1 [mysqld]
        2 slow_query_log = ON
        3 slow_query_log_file = /usr/local/mysql/data/slow.log
        4 long_query_time = 2
    但这种方式需要重启 mysql服务
### 34.1.2 监控
     Metrics - 监控指标
        系统性能指标，包括请求成功率、系统吞吐量、响应时长
        资源性能指标，衡量系统软硬件资源使用情况，配合系统性能指标，观察系统资源水位
     Logs - 日志
        施压引擎日志，观察施压引擎是否健康，压测脚本执行是否有报错
        采样日志，采样记录 API 的请求和响应详情，辅助排查压测过程中的一些出错请求的参数是否正常，并通过响应详情，查看完整的错误信息
     Traces - 分布式链路追踪
        用于性能问题诊断阶段，通过追踪请求在系统中的调用链路，定位报错 API 的报错系统和报错堆栈，快速定位性能问题点。
## 34.3 链路追踪
    有时候某个接口涉及的逻辑很多，比如：查数据库、查 redis、远程调用接口，发 mq消息，执行业务代码等等。
    该接口一次请求的链路很长，如果逐一排查，需要花费大量的时间，这时候，我们已经没法用传统的办法定位问题了。有没有办法解决这问题呢？
    用分布式链路跟踪系统：skywalking。通过 skywalking 定位性能问题
    在 skywalking 中可以通过 traceId（全局唯一的 id），串联一个接口请求的完整链路。
    可以看到整个接口的耗时，调用的远程服务的耗时，访问数据库或者 redis 的耗时等等，功能非常强大。
    之前没有这个功能的时候，为了定位线上接口性能问题，我们还需要在代码中加日志，手动打印出链路中各个环节的耗时情况，然后再逐一排查。
## 34.4 问题排查
### 34.4.1 个别接口响应慢
#### 34.4.1.1 链路追踪
    使用 SkyWalking，展示出每一个与网络有关的耗时。比如：读写数据库、读写 Redis、
    SpringCloud 调用、Dubbo 调用等。这样就能立马定位是哪次操作耗时了。
    同时，SkyWalking 可以记录每一个 SQL 语句，可以帮助定位
## 34.5 垃圾回收
### 34.5.1 什么情况下，GC 会对程序产生影响？
    不管 Minor GC 还是 FGC，都会造成一定程度的程序卡顿（即 Stop The World：GC线程开始工作，其他工作线程被挂起），即使采用 ParNew、CMS 或者 G1 这些更先进的垃圾回收算法，也只是在减少卡顿时间，而并不能完全消除卡顿。
    那到底什么情况下，GC 会对程序产生影响呢？根据严重程度从高到底，包括以下 4 种情况：
    1. FGC 过于频繁
        a. FGC 通常比较慢，少则几百毫秒，多则几秒，正常情况 FGC 每隔几个小时甚至几天才执行一次，对系统的影响还能接受。
        b. 一旦出现 FGC 频繁（比如几十分钟执行一次），是存在问题的，会导致工作线程频繁被停止，让系统看起来一直有卡顿现象，也会使得程序的整体性能变差。
    2. YGC 耗时过长
        执行频率高：通常几秒到几分钟就会发生一次
        a. 一般来说，YGC 的总耗时在几十或者上百毫秒是比较正常的，虽然会引起系统卡顿几毫秒或者几十毫秒，这种情况几乎对用户无感知，对程序的影响可以忽略不计。
        b. 如果 YGC 耗时达到了 1 秒甚至几秒（都快赶上 FGC 的耗时了），那卡顿时间就会增大，加上 YGC 本身比较频繁，就会导致比较多的服务超时问题。
    3. FGC 耗时过长
        a. FGC 耗时增加，卡顿时间也会随之增加，尤其对于高并发服务，可能导致 FGC期间比较多的超时问题，可用性降低，这种也需要关注。
    4. YGC 过于频繁
        a. 即使 YGC 不会引起服务超时，但是 YGC 过于频繁也会降低服务的整体性能，对于高并发服务也是需要关注的。
### 34.5.2 如何排查
    1. 公司的监控系统：大部分公司都会有，可全方位监控 JVM 的各项指标。
    2. JDK 自带工具：jmap、jstat
        a. 查看堆内存各区域的使用率以及 GC 情况：jstat -gcutil pid 1000 （重点关注结果中的 YGC、YGCT、FGC、FGCT、GCT）
        b. 查看堆内存中的存活对象，并按空间排序：jmap -histo pid | head -n20
        c. dump 堆内存文件：jmap -dump:format=b,file=heap pid

# 35. 百万级别数据的 Excel 如何快速导入到数据库中
    详见https://github.com/chasesunshine/mjj-test-new-technology/demo-mybatis-plus11/doc/Spring Boot 多线程导入百万级Excel数据到数据库.md
## 35.6 总结
    提升 Excel 导入速度的方法：
    1.使用更快的 Excel 读取框架(推荐使用阿里 EasyExcel)
    2.对于需要与数据库交互的校验、按照业务逻辑适当的使用缓存。用空间换时间
    3.使用 values(),(),() 拼接长 SQL 一次插入多行数据
    4.使用多线程插入数据，利用掉网络 IO 等待时间(推荐使用并行流，简单易用)
    5.避免在循环中打印无用的日志

# 38. 系统升级过程中如何实现数据的平滑迁移
## 38.1 如何平滑地迁移数据库中的数据
    你可能会认为：数据迁移无非是将数据从一个数据库拷贝到另一个数据库，可以通过MySQL 主从同步的方式做到准实时的数据拷贝；也可以通过 mysqldump 工具将源库的数据导出，再导入到新库，这有什么复杂的呢？
    其实，这两种方式只能支持单库到单库的迁移，无法支持单库到多库多表的场景。
### 38.1.1 “双写”方案
    1. 将新的库配置为源库的从库，用来同步数据；如果需要将数据同步到多库多表，那么可以使用一些第三方工具获取 Binlog 的增量日志（比如开源工具 Canal），在获取增量日志之后就可以按照分库分表的逻辑写入到新的库表中了。
    2. 同时，我们需要改造业务代码，在数据写入的时候，不仅要写入旧库，也要写入新库。当然，基于性能的考虑，我们可以异步地写入新库，只要保证旧库写入成功即可。但是，我们需要注意的是，需要将写入新库失败的数据记录在单独的日志中，
        这样方便后续对这些数据补写，保证新库和旧库的数据一致性。
    3. 然后，我们就可以开始校验数据了。由于数据库中数据量很大，做全量的数据校验不太现实。你可以抽取部分数据，具体数据量依据总体数据量而定，只要保证这些数据是一致的就可以。
    4. 如果一切顺利，我们就可以将读流量切换到新库了。由于担心一次切换全量读流量可能会对系统产生未知的影响，所以这里最好采用灰度的方式来切换，比如开始切换
        10% 的流量，如果没有问题再切换到 50% 的流量，最后再切换到 100%。
    5. 由于有双写的存在，所以在切换的过程中出现任何的问题，都可以将读写流量随时切换到旧库去，保障系统的性能。
    6. 在观察了几天发现数据的迁移没有问题之后，就可以将数据库的双写改造成只写新库，数据的迁移也就完成了。
### 38.1.2 级联同步方案
    我们会在自建机房准备一个备库，在云上环境上准备一个新库，通过级联同步的方式在自建机房留下一个可回滚的数据库，具体的步骤如下：
    1. 先将新库配置为旧库的从库，用作数据同步；
    2. 再将一个备库配置为新库的从库，用作数据的备份；
    3. 等到三个库的写入一致后，将数据库的读流量切换到新库；
    4. 然后暂停应用的写入，将业务的写入流量切换到新库（由于这里需要暂停应用的写入，所以需要安排在业务的低峰期）。

# 39. 如何确保你的消息只被消费一次？
## 39.1 消息为什么会丢失
    如果要保证消息只被消费一次，首先就要保证消息不会丢失。那么消息从被写入到消息队列，到被消费者消费完成，这个链路上会有哪些地方存在丢失消息的可能呢？其实，主要存在三个场景：
        消息从生产者写入到消息队列的过程。
        消息在消息队列中的存储场景。
        消息被消费者消费的过程。
### 39.1.1 在消息生产的过程中丢失消息
    在这个环节中主要有两种情况。
    首先，消息的生产者一般是我们的业务服务器，消息队列是独立部署在单独的服务器上的。两者之间的网络虽然是内网，但是也会存在抖动的可能，而一旦发生抖动，消息就有可能因为网络的错误而丢失。
    针对这种情况，我建议你采用的方案是消息重传：也就是当你发现发送超时后你就将消息重新发一次，但是你也不能无限制地重传消息。一般来说，如果不是消息队列发生故障，或者是到消息队列的网络断开了，重试 2～3 次就可以了
    不过，这种方案可能会造成消息的重复，从而导致在消费的时候会重复消费同样的消息。比方说，消息生产时由于消息队列处理慢或者网络的抖动，导致虽然最终写入消息队列成功，但在生产端却超时了，
    生产者重传这条消息就会形成重复的消息，那么针对上面的例子，直观显示在你面前的就会是你收到了两个现金红包。
    那么消息发送到了消息队列之后是否就万无一失了呢？当然不是，在消息队列中消息仍然有丢失的风险。
### 39.1.2 在消息队列中丢失消息
    我给你的建议是：
    1. 如果你需要确保消息一条都不能丢失，那么建议不要开启消息队列的同步刷盘，而是需要使用集群的方式来解决，可以配置当所有 ISR Follower 都接收到消息才返回成功。
    2. 如果对消息的丢失有一定的容忍度，那么建议不部署集群，即使以集群方式部署，也建议配置只发送给一个 Follower 就可以返回成功了。
    3. 我们的业务系统一般对于消息的丢失有一定的容忍度，比如说以上面的红包系统为例，如果红包消息丢失了，我们只要后续给没有发送红包的用户补发红包就好了。
### 39.1.3 在消费的过程中存在消息丢失的可能
    所以，在这里你需要注意的是，一定要等到消息接收和处理完成后才能更新消费进度，但是这也会造成消息重复的问题，比方说某一条消息在处理之后，消费者恰好宕机了，那么因为没有更新消费进度，所以当这个消费者重启之后，还会重复地消费这条消息。
## 39.2 如何保证消息只被消费一次
    想要完全的避免消息重复的发生是很难做到的，因为网络的抖动、机器的宕机和处理的异常都是比较难以避免的，在工业上并没有成熟的方法，因此我们会把要求放宽，只要
    保证即使消费到了重复的消息，从消费的最终结果来看和只消费一次是等同的就好了，也就是保证在消息的生产和消费的过程是“幂等”的。
### 39.2.1 什么是幂等
    幂等是一个数学上的概念，它的含义是多次执行同一个操作和执行一次操作，最终得到的结果是相同的
    说白了，你可以这么理解“幂等”：一件事儿无论做多少次都和做一次产生的结果是一样的，那么这件事儿就具有幂等性。
### 39.2.2 在生产、消费过程中增加消息幂等性的保证
    在消息生产过程中，在 Kafka0.11 版本和 Pulsar 中都支持“produceridempotency”的特性，翻译过来就是生产过程的幂等性，这种特性保证消息虽然可能在生产端产生重复，但是最终在消息队列存储时只会存储一份。
    它的做法是给每一个生产者一个唯一的 ID，并且为生产的每一条消息赋予一个唯一ID，消息队列的服务端会存储 < 生产者 ID，最后一条消息 ID> 的映射。当某一个生产者产生新的消息时，消息队列服务端会比对消息 ID 是否与存储的最后一条 ID 一致，
        如果一致，就认为是重复的消息，服务端会自动丢弃。
    而在消费端，幂等性的保证会稍微复杂一些，你可以从通用层和业务层两个层面来考虑。在通用层面，你可以在消息被生产的时候，使用发号器给它生成一个全局唯一的消息ID，消息被处理之后，把这个 ID 存储在数据库中，
        在处理下一条消息之前，先从数据库里面查询这个全局 ID 是否被消费过，如果被消费过就放弃消费
    你可以看到，无论是生产端的幂等性保证方式，还是消费端通用的幂等性保证方式，它们的共同特点都是为每一个消息生成一个唯一的 ID，然后在使用这个消息的时候，
        先比对这个 ID 是否已经存在，如果存在，则认为消息已经被使用过。所以这种方式是一种标准的实现幂等的方式
    不过这样会有一个问题：如果消息在处理之后，还没有来得及写入数据库，消费者宕机了重启之后发现数据库中并没有这条消息，还是会重复执行两次消费逻辑，这时你就需要引入事务机制，保证消息处理和写入数据库必须同时成功或者同时失败，
        但是这样消息处理的成本就更高了，所以，如果对于消息重复没有特别严格的要求，可以直接使用这种通用的方案，而不考虑引入事务
    在业务层面怎么处理呢？这里有很多种处理方式，其中有一种是增加乐观锁的方式。比如，你的消息处理程序需要给一个人的账号加钱，那么你可以通过乐观锁的方式来解决。具体的操作方式是这样的：你给每个人的账号数据中增加一个版本号的字段，
        在生产消息时先查询这个账户的版本号，并且将版本号连同消息一起发送给消息队列。消费端在拿到消息和版本号后，在执行更新账户金额 SQL 的时候带上版本号，类似于执行：
        update user set amount = amount + 20, version=version+1 where userId=1 and version=1;
    你看，我们在更新数据时给数据加了乐观锁，这样在消费第一条消息时，version 值为1，SQL 可以执行成功，并且同时把 version 值改为了 2；在执行第二条相同的消息时，由于 version 值不再是 1，所以这条 SQL 不能执行成功，也就保证了消息的幂等性。

# 40. 如何设计一个在生产环境下做全链路压测的平台
## 40.1 什么是压力测试
    不过，我想让你回想一下，自己是怎么做压力测试的？是不是像很多同学一样：先搭建一套与正式环境功能相同的测试环境，并且导入或者生成一批测试数据，然后在另一台服务器，启动多个线程并发地调用需要压测的接口（接口的参数一般也会设置成相同的，
        比如，想要压测获取商品信息的接口，那么压测时会使用同一个商品 ID）。最后，通过统计访问日志，或者查看测试环境的监控系统，来记录最终压测 QPS 是多少之后，直接交差？
    这么做压力测试其实是不正确的，错误之处主要有以下几点：
    1. 首先，做压力测试时，最好使用线上的数据和线上的环境，因为，你无法确定自己搭建的测试环境与正式环境的差异，是否会影响到压力测试的结果；
    2. 其次，压力测试时不能使用模拟的请求，而是要使用线上的流量。你可以通过拷贝流量的方式，把线上流量拷贝一份到压力测试环境。因为模拟流量的访问模型，和线上流量相差很大，会对压力测试的结果产生比较大的影响。
        比如，你在获取商品信息的时候，线上的流量会获取不同商品的数据，这些商品的数据有的命中了缓存，有的没有命中缓存。如果使用同一个商品 ID 来做压力测试，那么只有第一次请求没有命中缓存，
        而在请求之后会将数据库中的数据回种到缓存，后续的请求就一定会命中缓存了，这种压力测试的数据就不具备参考性了。
    3. 不要从一台服务器发起流量，这样很容易达到这台服务器性能瓶颈，从而导致压力测试的 QPS 上不去，最终影响压力测试的结果。而且，为了尽量真实地模拟用户请求，我们倾向于把流量产生的机器，放在离用户更近的位置，
        比如放在 CDN 节点上。如果没有这个条件，那么可以放在不同的机房中，这样可以尽量保证压力测试结果的真实性。
    之所以有很多同学出现这个问题，主要是对压力测试的概念没有完全理解，以为只要是使用多个线程并发的请求服务接口，就算是对接口进行压力测试了。
    那么究竟什么是压力测试呢？压力测试指的是，在高并发大流量下，进行的测试，测试人员可以通过观察系统在峰值负载下的表现，从而找到系统中存在的性能隐患。与监控一样，压力测试是一种常见的，发现系统中存在问题的方式，
        也是保障系统可用性和稳定性的重要手段。而在压力测试的过程中，我们不能只针对某一个核心模块来做压测，而需要将接入层、所有后端服务、数据库、缓存、消息队列、中间件以及依赖的第三方服务系统及其资源，都纳入压力测试的目标之中。
        因为，一旦用户的访问行为增加，包含上述组件服务的整个链路都会受到不确定的大流量的冲击，因此，它们都需要依赖压力测试来发现可能存在的性能瓶颈，这种针对整个调用链路执行的压力测试也称为“全链路压测”。
## 40.2 如何搭建全链路压测平台
    搭建全链路压测平台，主要有两个关键点。
    一点是流量的隔离。由于压力测试是在正式环境进行，所以需要区分压力测试流量和正式流量，这样可以针对压力测试的流量做单独的处理。
    另一点是风险的控制。也就是，尽量避免压力测试对于正常访问用户的影响，
    因此，一般来说全链路压测平台需要包含以下几个模块：
        流量构造和产生模块；
        压测数据隔离模块；
        系统健康度检查和压测流量干预模块。

# 41. [线上问题排查]MQ 消息队列消息堆积问题排查和解决思路
    MQ 执行有三大阶段：
         消息生产阶段。
         消息存储阶段。
         消息消费阶段。
    很显然，消息堆积是出现在第三个消息消费阶段的。
## 41.2 排查以及解决
### 41.2.1 方案一 增消费者
    第一我们想到的原因，流量激增，生成的订单速度远远大于消费者消费消息的速度，目前我们只部署了三个节点，那我们是否增加消费者，就可以解决这个问题，让消费者消费消息的速度远远大于生成者生成消息的速度，
        那消息就不存在堆积的问题，自然服务器压力也就下来了
    通知运维，再部署三个点，也是就增加三个消费者，由原来的三个消费者变为 6 个消费者，信心满满的部署完成后，等待一段时间，不出意外还是出了意外，消息还是在持续堆积，没有任何改善，我心里那个急啊，为什么增加了消费者？一点改善没有呢
### 41.2.2 方案二 优化消费者的处理逻辑
    消费者逻辑优化，屏蔽掉调用库存的接口，直接处理消息，但这种我们的逻辑是不完成，虽然能减少服务器的压力，后续处理起来也非常的麻烦，这种方式不可取
### 41.2.3 方案三 清空堆积的消息
    为了减少消息的堆积，减轻服务器的压力，我们是否可以把 mq 里面的消息拿出来，先存储，等服务恢复后，再把存储的消息推送到 mq，再处理呢？
    新建消费者，消费 rabbitmq 的消息，不做任何业务逻辑处理，直接快速消费消息，把消息存在一张表里，这样就没消息的堆积，服务器压力自然就下来了
    这方案上线后，过了一段时间观察，消息不再堆积，服务器的负载也下来了，我内心也不再慌了，那存储的那些消息，还处理吗？当然处理，怎么处理呢？
    后续等库存服务问题解决后，停掉新的消费者，新建一个生产者，再把表里的订单数据推送到 rabbitmq，进行业务逻辑的处理
## 41.3 消息堆积为什么会导致 cpu 飙升呢？
    RabbitMQ 是一种消息中间件，用于在应用程序之间传递消息。当消息堆积过多时，可能会导致 CPU
    1. 消息过多导致消息队列堆积：当消息的产生速度大于消费者的处理速度时，消息会积累在消息队列中。如果消息堆积过多，RabbitMQ 需要不断地进行消息的存储、检索和传递操作，这会导致 CPU 使用率升
    2. 消费者无法及时处理消息：消费者处理消息的速度不足以追赶消息的产生速度，导致消息不断积累在队列中。这可能是由于消费者出现瓶颈，无法处理足够多的消息，或者消费者的处理逻辑复杂，导致消费过程耗费过多的 CPU 资源
    3. 消息重试导致额外的 CPU 开销：当消息处理失败时，消费者可能会进行消息的重试操作，尝试再次处理消息。如果重试频率较高，会导致消息在队列中频繁流转、被重复消费，这会增加额外的 CPU 开销。
    4. 过多的连接以及网络 IO：当消息堆积过多时，可能会引发大量的连接请求和网络数据传输。这会增加网络 IO
## 41.4 通用的解决方案
     增加消费者：通过增加消费者的数量来提升消息的处理能力。增加消费者可以分担消息消费的负载，缓解消息队列的堆积问题。
     优化消费者的处理逻辑：检查消费者的代码是否存在性能瓶颈或是复杂的处理逻辑。可以通过优化算法、减少消费过程的计算量或是提高代码的效率来减少消费者的 CPU 开销。
     避免频繁的消息重试：当消息无法处理时，可以根据错误类型进行不同的处理方式，如将无法处理的消息转移到死信队列中或进行日志记录。避免频繁地对同一消息进行重试，以减少额外的 CPU 开销。
     调整 RabbitMQ 配置：可以调整 RabbitMQ 的参数来适应系统的需求，如增加内存、调整消息堆积的阈值和策略，调整网络连接等配置。
     扩展硬件资源：如果以上措施无法解决问题，可能需要考虑增加 RabbitMQ 的集群节点或者扩容服务器的硬件资源，以提升整个系统的处理能力。

# 42. [线上问题排查]如果 JVM 出现频繁 FullGC该如何解决
    我们知道 Full GC 的触发条件大致情况有以下几种情况：
        1. 程序执行了 System.gc() //建议 jvm 执行 fullgc，并不一定会执行
        2. 执行了 jmap -histo:live pid 命令 //这个会立即触发 fullgc
        3. 在执行 minor gc 的时候进行的一系列检查
            执行 Minor GC 的时候，JVM 会检查老年代中最大连续可用空间是否大于了当前新生代所有对象的总大小。
            如果大于，则直接执行 Minor GC（这个时候执行是没有风险的）。
            如果小于了，JVM 会检查是否开启了空间分配担保机制，如果没有开启则直接改为执行 Full GC。
            如果开启了，则 JVM 会检查老年代中最大连续可用空间是否大于了历次晋升到老年代中的平均大小，如果小于则执行改为执行 Full GC。
            如果大于则会执行 Minor GC，如果 Minor GC 执行失败则会执行 Full GC
        （马佳健个人理解：因为新生代中对象有可能全部都晋升到老年代，所以需要判断老年代中的连续最大空间是否大于新生代所有对象的总大小）
        4. 使用了大对象 //大对象会直接进入老年代
        5. 在程序中长期持有了对象的引用 //对象年龄达到指定阈值也会进入老年代对于我们的情况，可以初步排除 1，2 两种情况，最有可能是 4 和 5 这两种情况。为了进一步排查原因，我们在线上开启了 -XX:+HeapDumpBeforeFullGC
            注意：
            JVM 在执行 dump 操作的时候是会发生 stop the word 事件的，也就是说此时所有的用户线程都会暂停运行。
            为了在此期间也能对外正常提供服务，建议采用分布式部署，并采用合适的负载均衡算法

# 43. [线上问题排查]JVM OOM 问题如何排查和解决
    五种常见的 oom 错误
         java.lang.OutOfMemoryError: Java heap space
         java.lang.OutOfMemoryError: unable to create new native thread
         java.lang.OutOfMemoryError: Metaspace
         java.lang.OutOfMemoryError: Direct buffer memory
         java.lang.OutOfMemoryError: GC overhead limit
## 43.1 通用手段
### 43.1.1 发现
    通过以下手段可以做到感知应该出现了 oom.
    1. 日志监控
        通过监控日志中关键字 java.lang.OutOfMemoryError，就可以知道应用是否出现oom.对于文件的监控，可以使用 filebeat 或者 flume 等采集
    2. 配置-XX:+ExitOnOutOfMemoryError
        当 jvm 启用该参数时，如果出现了 oom，就会自动退出程序，咱们的健康检测自然能发现应用不存在了，从而能发出告警
    3. 使用 jstat 监控 jvm 内存使用率
        通过 jstat 工具可以查看 jvm 的内存使用情况，如果老年代使用率一直是 100%，并且期间还在一直不断 GC，这也是发生了 oom 的一种现象。jstat -gcutil $pid 1000 使用该命令即可实现每秒打印一次 jvm 的内存信息。
        O 列代表老年代内存使用比例，YGC 和 FGC 分别代表新生代 GC 次数和老年代 GC 次数。
### 43.1.2 止血
    当应用出现 oom 时，意味着应用无法申请到内存来分配对象，那么咱们的业务代码就会处于混沌状态，不清楚能进行到哪一步，不清楚状态是否完备，业务的请求可能在系统无限阻塞。这种情况对于用户体验的伤害会非常大。
    所以这里有一个简单的方法就是，配置-XX:+HeapDumpOnOutOfMemoryError 参数，当应用发生 oom 后，自动关停，借助 k8s 的健康检测实现自动重启能力再次拉起来新的容器。
        注意，这里需要实现业务系统能够实现故障转移或者请求幂等。这样才能保证用户请求不会出问题。
## 43.2 解决方案
    对于 unable to create new native thread 、Metaspace、Direct buffer memory 和 GC overhead limit exceeded,它们的原因相对简单，可以直接给出结论。
     unable to create new native thread : 无法创建更多的操作系统线程，以下 3 种情况:
         说明当前系统的线程数过多，超过了系统线程数的上限，减少当前应用创建的线程即可。
         没有 native 内存
     Metaspace：
         说明 Metaspace 不够用，修改 -XX:MaxMetaspaceSize 启动参数，调大永久代空间即可
     Direct buffer memory：
         Direct ByteBuffer 的默认大小为 64 MB，一旦使用超出限制，就会抛出Directbuffer memory 错误
     GC overhead limit exceeded：
         当 jvm 98%的时间都在 GC 时，就会出现该异常。这种情况需要增大堆或者结合下面的方法继续排查。
### 43.2.1 分析
    当应用发生 oom 后，最佳的分析载体就是 heap dump。通过添加-XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/tmp/app.dump参数，可以实现当应用发生 oom 时，全自动导出 heap dump。
    有了 heap dump，我们就可以借助 Eclipse MAT 这款工具来分析 heapdump 了
### 43.2.2 规避
    oom 发生的原因有两种:
        1. 应用正常，但是堆设置过小，无法支持正常的对象分配
        2. 应用发生了内存泄漏，导致应该被清理的对象没有清理，无限增长通过分析 heap dump，我们基本可以区分出两种情况，
            第一种情况，对象的分布都正确，没有那种以下占用 30%*-，甚至 50%的对象。第二种情况就是某一种类型的对象，占据了大量的内存空间。
            第一种情况的解决方案就是：
                3. 增大堆内存
                4. 优化应用内存使用效率
            第二种情况，就需要针对性分析了，这里笔者给出常见的 oom 原因，大家在问题排查时，可以逐个排除，直到找到正确答案：
                5. ThreadLocal 未清理
                6. 出现了未指定分页的大数据量查询
                7. 定时任务中 list 忘记清空，每次都追加数据
                8. 监控系统中使用了不可控的字段作为 label,导致 label 无限增长

# 44. [线上问题排查]CPU 使用率较高排查和解决思路
## 44.1 问题发现
   本文整理自一个真实的案例，是楼主负责的业务，在一次大促之前的压测时发现了这个问题。
   在每次大促之前，我们的测试人员都会对网站进行压力测试，这个时候会查看服务的cpu、内存、load、rt、qps等指标。
   在一次压测过程中，测试人员发现我们的某一个接口，在qps上升到500以后，CPU使用率急剧升高。
## 44.2 问题定位
   遇到这种问题，首先是登录到服务器，看一下具体情况
### 44.2.1 定位进程
    登录服务器，执行top命令，查看CPU占用情况：
    $top
     PID USER      PR NI VIRT RES SHRS% CPU%  MEM TIME+COMMAND
     1893 admin     20 07127m2.6g 38mS  181.7 32.6 10:20.26 java
     top 命令是Linux下常用的性能分析工具，能够实时显示系统中各个进程的资源占用状况，类似于Windows的任务管理器。
     通过以上命令，我们可以看到，进程ID为1893的Java进程的CPU占用率达到了181%，基本可以定位到是我们的Java应用导致整个服务器的CPU占用率飙升。
### 44.2.2 定位线程
    我们知道，Java是单进程多线程的，那么，我们接下来看看PID=1893的这个Java进程中的各个线程的CPU使用情况，同样是用top命令：
    $top-Hp 1893
         PID USER   PR NI VIRT RES SHRS  %CPU  %MEM     TIME+COMMAND
         4519 admin 20 07127m2.6g  38mR  18.6  32.6      0:40.11 java
    通过top-Hp1893命令，我们可以发现，当前1893这个进程中，ID为4519的线程占用CPU最高。
### 44.2.3 定位代码
    通过top命令，我们目前已经定位到导致CPU使用率较高的具体线程，那么我么接下来就定位下到底是哪一行代码存在问题。
    首先，我们需要把4519这个线程转成16进制：
    $printf %x 4519
    11a7
    接下来，通过jstack命令，查看栈信息：
     1 $sudo-u admin jstack 1893 |grep-A 200 11a7
     2 "HSFBizProcessor-DEFAULT-8-thread-5" #500 daemon prio=10 os_prio=0 tid=0x00007f632314a800 nid=0x11a2 runnable [0x000000005442a000]
     3 java.lang.Thread.State: RUNNABLE
     4 atsun.misc.URLClassPath$Loader.findResource(URLClassPath.java:684)
     5 atsun.misc.URLClassPath.findResource(URLClassPath.java:188)
     6 atjava.net.URLClassLoader$2.run(URLClassLoader.java:569)
     7 atjava.net.URLClassLoader$2.run(URLClassLoader.java:567)
     8 atjava.security.AccessController.doPrivileged(Native Method)
     9 atjava.net.URLClassLoader.findResource(URLClassLoader.java:566)
     10 atjava.lang.ClassLoader.getResource(ClassLoader.java:1093)
     11 atjava.net.URLClassLoader.getResourceAsStream(URLClassLoader.java:232)
     12 at org.hibernate.validator.internal.xml.ValidationXmlParser.getInputStreamForPath(ValidationXmlParser.java:248)
     13 at org.hibernate.validator.internal.xml.ValidationXmlParser.getValidationConfig(ValidationXmlParser.java:191)
     14 at org.hibernate.validator.internal.xml.ValidationXmlParser.parseValidationXml(ValidationXmlParser.java:65)
     15 at org.hibernate.validator.internal.engine.ConfigurationImpl.parseValidationXml(ConfigurationImpl.java:287)
     16 at org.hibernate.validator.internal.engine.ConfigurationImpl.buildValidatorFactory(ConfigurationImpl.java:174)
     17 atjavax.validation.Validation.buildDefaultValidatorFactory(Validation.java:111)
     18 at com.test.common.util.BeanValidator.validate(BeanValidator.java:30)
    通过以上代码，我们可以清楚的看到，BeanValidator.java的第30行是有可能存在问题的。
## 44.3 问题解决
   接下来就是通过查看代码来解决问题了，我们发现，我们自定义了一个BeanValidator，封装了Hibernate的Validator，然后在validate方法中，
    通过Validation.buildDefaultValidatorFactory().getValidator()初始化一个 Validator 实例，通过分析发现这个实例化的过程比较耗时。
   我们重构了一下代码，把Validator实例的初始化提到方法外，在类初始化的时候创建一次就解决了问题。
## 44.4 总结
   以上，展示了一次比较完成的线上问题定位过程。主要用到的命令有:top 、printf 和jstack
   另外，线上问题排查还可以使用Alibaba开源的工具Arthas进行排查，以上问题，可以使用一下命令定位：thread-n 3 //查看cpu占比前三的线程
   以上，本文介绍了如何排查线上服务器CPU使用率过高的问题，如果大家感兴趣，后面可以再介绍一些关于LOAD飙高、频繁GC等问题的排查手段。

# 45. [合集]海量数据下的数据统计高频面试题系列
## 45.1 [百度二面]内存100M下统计不同号码的个数
    已知某个文件内包含大量电话号码，每个号码为8位数字，如何统计不同号码的个数？内存限制100M
    个人做法：
        1. 如果号码数量在200万以内：使用HashSet最简单直接
        2. 如果号码数量巨大但可接受近似结果：使用布隆过滤器 BloomFilter
        3. 如果数据量极大且需要精确结果：使用外部排序方法
        4. 如果号码是数字且有固定模式：考虑位图法 BitSet
##  45.2 [美团一面]出现频率最高的100个词
    假如有一个1G大小的文件，文件里每一行是一个词，每个词的大小不超过16byte，要求返回出现频率最高的100个词。内存大小限制是10M
        第一步：分割大文件为多个小文件并分别统计词频
        第二步：合并所有小文件的统计结果
        第三步：使用PriorityQueue最小堆找出Top K
## 45.3 [字节一面]查找两个大文件共同的URL
    给定 a、b 两个文件，各存放 50 亿个 URL，每个 URL 各占 64B，找出 a、b两个文件共同的 URL。内存限制是 4G
    解决方案：分治+哈希
    步骤1：文件分片
        使用哈希函数将URL分配到不同的小文件中：
        对每个URL计算哈希值，如hash(URL) % 1000
        根据哈希值将URL分配到a0-a999和b0-b999的小文件中
        这样相同的URL会被分配到相同编号的小文件中
    步骤2：逐对处理小文件
        对于每一对相同编号的小文件(ai, bi)：
        将ai的URL加载到HashSet中（内存中可以容纳）
        遍历bi中的URL，检查是否存在于ai的HashSet中
        输出共同的URL
## 45.6 [大众点评]如何找出排名前 500 的数？
    有 1w 个数组，每个数组有 500 个元素，并且有序排列。如何在这 10000*500个数中找出前 500 的数？
    马佳健个人思路：
        1. 新建一个 PriorityQueue<Map.Entry<String, Integer>> minHeap = new PriorityQueue<>(Comparator.comparingInt(Map.Entry::getValue));
            这么一个优先级队列，按照值的大小进行排序
        2. 遍历这 1w 个数组
            遍历每个数组，放入 minHeap 中，它会按照值进行放入和排序，如果超过500个数，就一直offer，一直poll出去，直到遍历完成（以下是思路）
            minHeap.offer(entry);
            // 这地方是一旦超过了 k ，就在队列中把最小的给弹出
            if (minHeap.size() > k) {
                minHeap.poll();
            }

# 46. [线上问题排查]数据库出现死锁如何排查
    所以，面对线上偶发的 MySQL 死锁问题，我的排查处理过程如下：
    1. 线上错误日志报警发现死锁异常
    2. 查看错误日志的堆栈信息
    3. 查看 MySQL 死锁相关的日志
    4. 根据 binlog 查看死锁相关事务的执行内容
    5. 根据上述信息找出两个相互死锁的事务执行的 SQL 操作，根据本系列介绍的锁相关理论知识，进行分析推断死锁原因
    6. 修改业务代码
## 45.1 死锁日志的获取
    发生死锁异常后，我们可以直接使用 showengineinnodbstatus 命令获取死锁信息，但是该命令只能获取最近一次的死锁信息。所以，我们可以通过开启 InnoDB 的
        监控机制来获取实时的死锁信息，它会周期性（每隔 15 秒）打印 InnoDb 的运行状态到 mysqld 服务的错误日志文件中。
    InnoDb 的监控较为重要的有标准监控（StandardInnoDBMonitor）和 锁监控（InnoDB Lock Monitor），通过对应的系统参数可以将其开启。
        1-- 开启标准监控
        2 set GLOBAL innodb_status_output=ON;
        3-- 关闭标准监控
        4 set GLOBAL innodb_status_output=OFF;
        5-- 开启锁监控
        6 set GLOBAL innodb_status_output_locks=ON;
        7-- 关闭锁监控
        8 set GLOBAL innodb_status_output_locks=OFF;
    另外，MySQL 提供了一个系统参数 innodb_print_all_deadlocks 专门用于记录死锁日志，当发生死锁时，死锁日志会记录到 MySQL 的错误日志文件中。
         set GLOBAL innodb_print_all_deadlocks=ON;
## 45.3 binlog 的获取和分析
    binlog 日志会完整记录事务执行的所有 SQL，借助它，我们就能找到最终获取锁事务所执行的全部 SQL。然后再进行具体的锁冲突分析。
    我们可以使用 MySQL 的命令行工具Mysqlbinlog 远程获取线上数据库的 binlog日志。具体命令如下所示：
        cmd命令窗口：
            mysqlbinlog -h 127.0.0.1 -u root -p --read-from-remote-server binlog.0000056 --base64-output=decode-rows -v

# 47. 请你详细介绍一下扫码登录的实现原理？
    下面分阶段来看看设计原理。
    1、待扫描阶段
        首先是待扫描阶段，这个阶段是 PC 端跟服务端的交互过程。每次用户打开PC端登陆请求，系统返回一个唯一的二维码ID，并将二维码ID的信息绘制成二维码返回给用户。
        这里的二维码ID一定是唯一的，后续流程会将二维码ID跟身份信息绑定，不唯一的话就会造成你登陆了其他用户的账号或者其他用户登陆你的账号。此时在 PC 端会启动一个定时器，轮询查询二维码是否被扫描。如果移动端未扫描的话，那么一段时间后二维码将会失效。
    2、已扫描待确认阶段
        第二个阶段是已扫描待确认阶段，主要是移动端跟服务端交互的过程。
        首先移动端扫描二维码，获取二维码 ID，然后将手机端登录的凭证（token）和 二维码 ID 作为参数发送给服务端
        此时的手机在之前已经是登录的，不存在没登录的情况。
        服务端接受请求后，会将 token 与二维码 ID 关联，然后会生成一个临时token，这个 token 会返回给移动端，临时 token 用作确认登录的凭证。
        PC 端的定时器，会轮询到二维码的状态已经发生变化，会将 PC 端的二维码更新为已扫描，请在手机端确认。
    3、已确认
        扫码登录的最后阶段，用户点击确认登录，移动端携带上一步骤中获取的临时 token访问服务端。
        服务端校对完成后，会更新二维码状态，并且给 PC 端生成一个正式的 token。
        后续 PC 端就是持有这个 token 访问服务端。

# 49. [京东一面]如何用 Redis 统计用户访问量
    拼多多有数亿的用户，那么对于某个网页，怎么使用Redis来统计一个网站的用户访问数呢？
    1、Hash
        哈希是Redis的一种基础数据结构，Redis底层维护的是一个开散列，会把不同的key映射到哈希表上，如果是遇到关键字冲突，那么就会拉出一个链表出来
        当一个用户访问的时候，如果用户登陆过，那么我们就使用用户的id，如果用户没有登陆过，那么我们也能够前端页面随机生成一个key用来标识用户
        当用户访问的时候，我们可以使用HSET命令，key可以选择URI与对应的日期进行拼凑，field 可以使用用户的id或者随机标识，value可以简单设置为1。
        当我们要统计某一个网站某一天的访问量的时候，就可以直接使用HLEN来得到最终的结果了。
            [root@VM 0 2 centos ~]# redis-c
            redis-check-aof redis-check-rdbredis-cli
            [root@VM 0 2 centos ~]# redis-cli
            127.0.0.1:6379>hset index.html0527 11110 1
            (integer)1
            127.0.0.1:6379> hset index.html0527 11115 2
            (integer)1
            127.0.0.1:6379> hlen index.html0527
            (integer)2
        优点：简单，容易实现，查询也是非常方便，数据准确性非常高。
        缺点：占用内存过大。随着key的增多，性能也会下降。网站访问量不高还行，拼多多这种数亿PV的网站肯定顶不住。
    2、Bitset
        我们知道，对于一个32位的int，如果我们只用来记录id，那么只能够记录一个用户，但如果我们转成2进制，每位用来表示一个用户，那么我们就能够一口气表示32个用户，空间节省了32倍！
        对于有大量数据的场景，如果我们使用bitset，那么可以节省非常多的内存。
        对于没有登陆的用户，我们也可以使用哈希算法，把对应的用户标识哈希成一个数字id。bitset 非常的节省内存，假设有1亿个用户，也只需要100000000/8/1024/1024约等于12兆内存。
        Redis 已经为我们提供了SETBIT的方法，使用起来非常的方便，我们可以看看下面的例子。
        我们在item页面可以不停地使用SETBIT命令，设置用户已经访问了该页面，也可以使用GETBIT的方法查询某个用户是否访问。最后我们通过BITCOUNT可以统计该网页每天的访问数量。
            127.0.0.1:6379> setbit item.html0528 11111 1
            (integer)0
            127.0.0.1:6379>setbit item.html0528 11 1
            (integer)0
            127.0 0.1:6379> getbit item.html0528 33
            (integer)0
            127.0.0 1:6379> getbit item.html0528 11
            (integer)1
            127.0.0.1:6379> bitcount item.html0528
            (integer)2
        优点： 占用内存更小，查询方便，可以指定查询某个用户，数据可能略有瑕疵，对于非登陆的用户，可能不同的key映射到同一个id，否则需要维护一个非登陆用户的映射，有额外的开销。
        缺点： 如果用户非常的稀疏，那么占用的内存可能比方法一更大
        个人理解：
             redisTemplate.opsForValue().setBit("", 1, true)
             源码：               Boolean setBit(K key, long offset, boolean value);
             deepseek：
                 # RedisTemplate API 中的 setBit 方法含义
                 `Boolean setBit(K key, long offset, boolean value)` 是 Spring Data Redis 中 RedisTemplate 提供的一个方法，用于操作 Redis 的位图(bitmap)数据结构。
                 ## 方法参数解释
                     - `K key`: Redis 中位图的键名
                     - `long offset`: 位偏移量(从0开始)
                     - `boolean value`: 要设置的值(true表示1，false表示0)
                 ## 功能说明
                     这个方法用于对指定键的位图在指定偏移量处设置位值：
                     - 如果value为true，则将偏移量处的位设置为1
                     - 如果value为false，则将偏移量处的位设置为0
                 ## 返回值
                     该方法返回一个Boolean值，表示该偏移量处原来的位值：
                     - 返回true表示原来该位是1
                     - 返回false表示原来该位是0
                 ## 底层原理
                     这个方法对应于Redis的`SETBIT`命令，允许你对字符串值进行位级操作。Redis将字符串视为位数组，可以单独设置或获取每一位的值。
                 ## 使用示例
                     ```java
                     // 设置键"mybitmap"的第10位为1，并获取原来的值
                     Boolean previousValue = redisTemplate.opsForValue().setBit("mybitmap", 10, true);
                     // 设置键"flags"的第5位为0
                     redisTemplate.opsForValue().setBit("flags", 5, false);
                     ```
                 ## 注意事项
                     1. 如果键不存在，Redis会自动创建一个新的位图
                     2. 偏移量必须是非负整数
                     3. 位图会自动扩展以容纳设置的偏移量，中间未设置的位默认为0
                 位图常用于实现布隆过滤器、用户在线状态统计等场景。
    3、概率算法
        对于拼多多这种多个页面都可能非常多访问量的网站，如果所需要的数量不用那么准确，可以使用概率算法。
        事实上，我们对一个网站的UV的统计，1亿跟1亿零30万其实是差不多的。
        在Redis中，已经封装了HyperLogLog算法，他是一种基数评估算法。这种算法的特征，一般都是数据不存具体的值，而是存用来计算概率的一些相关数据。
            127.0.0.1:6379> pfadd pay.html0528 111 222 123 asb
            (integer)1
            127.0.0.1:6379> pfcount pay.html0528
            (integer)4
        当用户访问网站的时候，我们可以使用PFADD命令，设置对应的命令，最后我们只要通过PFCOUNT就能顺利计算出最终的结果，因为这个只是一个概率算法，所以可能存在0.81%的误差
        优点： 占用内存极小，对于一个key，只需要12kb。对于拼多多这种超多用户的特别适用。
        缺点： 查询指定用户的时候，可能会出错，毕竟存的不是具体的数据。总数也存在一定的误差。
        上面就是常见的3种适用Redis统计网站用户访问数的方法了。
            deepseek：
                # Redis PFADD 命令解析
                    您提供的命令是 Redis 的 HyperLogLog 数据结构操作命令：
                    ```
                    PFADD pay.html0528 111 222 123 asb
                    ```
                ## 命令含义
                    `PFADD` 是 Redis HyperLogLog 数据结构的添加命令，用于:
                    1. 向名为 `pay.html0528` 的 HyperLogLog 结构中添加元素 `111`, `222`, `123`, `asb`
                    2. 如果 HyperLogLog 不存在，会自动创建
                    3. 如果添加的元素导致基数估计发生变化，返回 1；否则返回 0
                ## HyperLogLog 特性
                    1. **高效统计不重复元素**：用于估计集合的基数(唯一元素数量)
                    2. **固定内存占用**：无论添加多少元素，只占用约12KB内存
                    3. **误差率约0.81%**：是概率性数据结构，不是精确计数
                ## 使用场景
                    - 统计网站每日独立访客(UV)
                    - 统计搜索关键词的不同数量
                    - 统计用户点击的不同商品ID数量
                ## 相关命令
                    - `PFCOUNT key`：获取基数估计值
                    - `PFMERGE destkey sourcekey [sourcekey...]`：合并多个HyperLogLog
                ## 注意事项
                    1. HyperLogLog 只提供基数估计，不存储实际元素
                    2. 无法获取已添加的元素列表
                    3. 适合大数据量下的近似统计，不适用于需要精确结果的场景
                您可以使用 `PFCOUNT pay.html0528` 来查看这个 HyperLogLog 中估计的唯一元素数量。

# 50. shoppe面试题实时订阅推送设计与实现
    什么是订阅推送？就是用户订阅了优惠劵的推送，在可领取前的一分钟就要把提醒信息推送到用户的app中。具体方案就是到具体的推送时间点了，coupon系统调用消息中心的推送接口，把信息推送出去。
    下面我们分析一下这个功能的业务情景。假设公司目前注册用户6000W+，比如有一张无门槛的优惠劵下单立减20元，那么抢这张劵的人就会比较多，我们保守估计10W+，百万级别不好说。
        我们初定为20W万人，那么这20W条推送信息要在一分钟推送完成！并且一个用户是可以订阅多张劵的。所以我们知道了这个订阅功能的有两个突出的难点：
        1、推送的实效性：推送慢了，用户会抱怨没有及时通知他们错过了开抢时机。
        2、推送的体量大：爆款的神劵，人人都想抢！
    推送的实效性的问题：当用户在领劵中心订阅了某个劵的领取提醒后，在后台就会生成一条用户的订阅提醒记录，里面记录了在哪个时间点给用户发送推送信息。所以问题就变成了系统如何快速实时选出哪些要推送的记录！



# 目前看到51. [阿里一面]购物车系统怎么设计？