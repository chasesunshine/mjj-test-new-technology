# 2. 电商平台中订单未支付过期如何实现自动关单？
## 定时任务 (个人不建议)
优点：
    实现容易，成本低，基本不依赖其他组件。
缺点：
    时间可能不够精确。由于定时任务扫描的间隔是固定的，所以可能造成一些订单已经过期了一段时间才被扫描到，订单关闭的时间比正常时间晚一些。
    增加了数据库的压力。随着订单的数量越来越多，扫描的成本也会越来越大，执行时间也会被拉长，可能导致某些应该被关闭的订单迟迟没有被关闭。
总结：
    采用定时任务的方案比较适合对时间要求不是很敏感，并且数据量不太多的业务场景。
## JDK 延迟队列 DelayQueue
    DelayQueue 是 JDK 提供的一个无界队列，我们可以看到，DelayQueue 队列中的元素需要实现 Delayed，它只提供了一个方法，就是获取过期时间
    用户的订单生成以后，设置过期时间比如 30 分钟，放入定义好的 DelayQueue，然后创建一个线程，在线程中通过 while(true)不断的从 DelayQueue 中获取过期的数据。
    优点：
        不依赖任何第三方组件，连数据库也不需要了，实现起来也方便。
    缺点：
        因为 DelayQueue 是一个无界队列，如果放入的订单过多，会造成 JVMOOM。DelayQueue 基于 JVM 内存，如果 JVM 重启了，那所有数据就丢失了。
    总结：
        DelayQueue 适用于数据量较小，且丢失也不影响主业务的场景，比如内部系统的一些非重要通知，就算丢失，也不会有太大影响
个人解惑:
    while(true)不断的从 DelayQueue 中获取过期的数据。用的是@PostConstruct之类的方案做处理
## redis 过期监听
    redis 是一个高性能的 KV 数据库，除了用作缓存以外，其实还提供了过期监听的功能。在 redis.conf 中，配置 notify-keyspace-events Ex 即可开启此功能。
    然后在代码中继承 KeyspaceEventMessageListener，实现 onMessage 就可以监听过期的数据量。
    优点：
        由于redis的高性能，所以我们在设置key，或者消费key时，速度上是可以保证的。
    缺点：
        由于redis的key过期策略原因，当一个key过期时，redis无法保证立刻将其删除，自然我们的监听事件也无法第一时间消费到这个key，所以会存在一定的延迟。
        另外，在redis5.0之前，订阅发布中的消息并没有被持久化，自然也没有所谓的确认机制。所以一旦消费消息的过程中我们的客户端发生了宕机，这条消息就彻底丢失了。
    总结：
        redis 的过期订阅相比于其他方案没有太大的优势，在实际生产环境中，用得相对较少。
## Redisson 分布式延迟队列
    Redisson 是一个基于 redis 实现的 Java 驻内存数据网格，它不仅提供了一系列的分布式的 Java 常用对象，还提供了许多分布式服务。
    Redisson 除了提供我们常用的分布式锁外，还提供了一个分布式延迟队列RDelayedQueue，他是一种基于 zset 结构实现的延迟队列，其实现类是RedissonDelayedQueue。
    优点：
        使用简单，并且其实现类中大量使用 lua 脚本保证其原子性，不会有并发重复问题。
    缺点：
        需要依赖 redis（如果这算一种缺点的话）。
    总结：
        Redisson 是 redis 官方推荐的 JAVA 客户端，提供了很多常用的功能，使用简单、高效，推荐大家尝试使用
## RocketMQ 延迟消息
    延迟消息，当消息写入到 Broker 后，不会立刻被消费者消费，需要等待指定的时长后才可被消费处理的消息，称为延时消息。
    在订单创建之后，我们就可以把订单作为一条消息投递到 rocketmq，并将延迟时间设置为 30 分钟，这样，30 分钟后我们定义的 consumer 就可以消费到这条消息，然后检查用户是否支付了这个订单
    优点：
        可以使代码逻辑清晰，系统之间完全解耦，只需关注生产及消费消息即可。另外其吞吐量极高，最多可以支撑万亿级的数据量。
    缺点：
        相对来说 mq 是重量级的组件，引入 mq 之后，随之而来的消息丢失、幂等性问题等都加深了系统的复杂度。
    总结：
        通过 mq 进行系统业务解耦，以及对系统性能削峰填谷已经是当前高性能系统的标配。
## RabbitMQ 死信队列
    除了 RocketMQ 的延迟队列，RabbitMQ 的死信队列也可以实现消息延迟功能。当 RabbitMQ 中的一条正常消息，因为过了存活时间（TTL 过期）、队列长度超限、被消费者拒绝等原因无法被消费时，就会被当成一条死信消息，投递到死信队列。
    基于这样的机制，我们可以给消息设置一个 ttl，然后故意不消费消息，等消息过期就会进入死信队列，我们再消费死信队列即可。通过这样的方式，就可以达到同 RocketMQ 延迟消息一样的效果。
    优点：
        同 RocketMQ 一样，RabbitMQ 同样可以使业务解耦，基于其集群的扩展性，也可以实现高可用、高性能的目标。
    缺点：
        死信队列本质还是一个队列，队列都是先进先出，如果队头的消息过期时间比较长，就会导致后面过期的消息无法得到及时消费，造成消息阻塞。
    总结：
        除了增加系统复杂度之外，死信队列的阻塞问题也是需要我们重点关注的。

# 3. 如何设计一个秒杀系统
    在我看来，秒杀系统本质上就是一个满足大并发、高性能和高可用的分布式系统
    在我看来，秒杀其实主要解决两个问题，一个是并发读，一个是并发写。
## 3.3.4 处理热点数据
    处理热点数据通常有几种思路：一是优化，二是限制，三是隔离。
    先来说说优化。优化热点数据最有效的办法就是缓存热点数据，如果热点数据做了动静分离，那么可以长期缓存静态数据。但是，缓存热点数据更多的是“临时”缓存，即不管是静态数据还是动态数据，都用一个队列短暂地缓存数秒钟，由于队列长度有限，可以采用 LRU 淘汰算法替换
    再来说说限制。限制更多的是一种保护机制，限制的办法也有很多，例如对被访问商品的 ID 做一致性 Hash，然后根据 Hash 做分桶，每个分桶设置一个处理队列，这样可以把热点商品限制在一个请求队列里，防止因某些热点商品占用太多的服务器资源，而使其他请求始终得不到服务器的处理资源。
    最后介绍一下隔离。秒杀系统设计的第一个原则就是将这种热点数据隔离出来，不要让1%的请求影响到另外的 99%，隔离出来后也更方便对这 1%的请求做针对性的优化。具体到“秒杀”业务，我们可以在以下几个层次实现隔离。
        1. 业务隔离。把秒杀做成一种营销活动，卖家要参加秒杀这种营销活动需要单独报名，从技术上来说，卖家报名后对我们来说就有了已知热点，因此可以提前做好预热。
        2. 系统隔离。系统隔离更多的是运行时的隔离，可以通过分组部署的方式和另外 99%分开。秒杀可以申请单独的域名，目的也是让请求落到不同的集群中。
        3. 数据隔离。秒杀所调用的数据大部分都是热点数据，比如会启用单独的 Cache 集群或者 MySQL 数据库来放热点数据，目的也是不想 0.01%的数据有机会影响99.99%数据。

# 4. 如果你的系统的 QPS 突然提升 10 倍你会怎么设计？
## 4.1 硬件的扩展+微服务的拆分
## 4.2 高性能 RPC
    有小伙伴进行对比测试，Dubbo RPC 的性能，是 Feign RPC 的性能 10 倍。(这一点 mjj 本人提出质疑)
    mjj个人搜索解答：
        Dubbo RPC 在性能上优于 Feign RPC，主要原因包括以下几个方面：
        1. **协议与传输层优化**
           - **Dubbo** 默认采用 **Dubbo 协议**，基于 **TCP 长连接** 和 **Netty** 进行通信，减少了连接建立和断开的开销，适合高并发场景。
           - **Feign** 基于 **HTTP/1.1**（短连接），每次请求都需要建立和断开 TCP 连接，增加了网络开销，性能较低。
        2. **序列化方式**
           - **Dubbo** 支持 **Hessian2、Protobuf** 等高效的二进制序列化方式，数据体积小，解析速度快。
           - **Feign** 默认使用 **JSON/XML** 等文本格式，数据体积较大，解析效率较低。
        3. **通信模型**
           - **Dubbo** 支持 **异步调用** 和 **NIO（Netty）**，能更好地利用系统资源，提高吞吐量。
           - **Feign** 基于 **同步阻塞式 HTTP 调用**，在高并发场景下容易出现性能瓶颈。
        4. **负载均衡与容错机制**
           - **Dubbo** 内置 **多种负载均衡策略**（随机、轮询、最少活跃调用等），并支持 **服务端和客户端负载均衡**，减少网络延迟。
           - **Feign** 依赖 **Ribbon（或 Spring Cloud LoadBalancer）**，仅支持客户端负载均衡，策略相对较少。
        5. **服务治理优化**
           - **Dubbo** 提供 **更细粒度的流量控制**（如并发控制、超时管理），减少无效请求，提高整体性能。
           - **Feign** 的容错依赖 **Hystrix（已停止维护）** 或 **Sentinel**，相比 Dubbo 的 **Failover/Failfast 等策略**，性能优化空间较小。
        6. **网络层优化**
           - **Dubbo** 的 **RPC 协议** 属于 **传输层**，比 **Feign（应用层 HTTP）** 减少了协议解析的开销。
           - **Dubbo 支持多协议**（如 gRPC、RMI），可根据业务需求选择最优方案。
        **总结**
        Dubbo 在 **协议优化、序列化效率、通信模型、负载均衡、服务治理** 等方面均优于 Feign，因此在高并发、低延迟场景下性能更高。而 Feign 的优势在于 **易用性、RESTful 兼容性**，适合中小规模微服务架构。
## 4.3 消息队列消峰解耦
## 4.4 三级缓存架构
    以秒杀系统举例，活动预热商品信息可以提前缓存提供查询服务，库存数据可以提前缓存，下单流程可以完全走缓存扣减，秒杀结束后再异步写入数据库，数据库承担的压力就小的太多了。
## 4.5 数据库分库分表
    读写分离也就相当于数据库集群的方式降低了单节点的压力。而面对数据的急剧增长，原来的单库单表的存储方式已经无法支撑整个业务的发展，这时候就需要对数据库进行分库分表了。针对微服务而言垂直的分库本身已经是做过的，剩下大部分都是分表的方案了。
## 4.6 高可用
## 4.7 总结
   其实可以看到，怎么设计高并发系统这个问题本身他是不难的，无非是基于你知道的知识点，从物理硬件层面到软件的架构、代码层面的优化，使用什么中间件来不断提高系统的抗压能力。
   但是这个问题本身会带来更多的问题，微服务本身的拆分带来了分布式事务的问题，http、RPC 框架的使用带来了通信效率、路由、容错的问题，MQ 的引入带来了消息丢失、积压、事务消息、顺序消息的问题，
   缓存的引入又会带来一致性、雪崩、击穿的问题，数据库的读写分离、分库分表又会带来主从同步延迟、分布式 ID、事务一致性的问题，而为了解决这些问题我们又要不断的加入各种措施熔断、限流、降级、离线核对、预案处理等等来防止和追溯这些问题。

# 6. 如何从零搭建 10 万级 QPS 大流量、高并发优惠券系统
### 6.3.1 存储瓶颈及解决方案
    瓶颈：
        在系统架构中，我们使用了 MySQL、Redis 作为存储组件。我们知道，单个服务器的 I/O 能力终是有限的，在实际测试过程中，能够得到如下的数据：
         单个 MySQL 的每秒写入在 4000 QPS 左右，超过这个数字，MySQL 的I/O 时延会剧量增长。
         MySQL 单表记录到达了千万级别，查询效率会大大降低，如果过亿的话，数据查询会成为一个问题。
         Redis 单分片的写入瓶颈在 2w
    解决方案：
        1. 读写分离。在查询券模板、查询券记录等场景下，我们可以将 MySQL 进行读写分离，让这部分查询流量走 MySQL 的读库，从而减轻 MySQL 写库的查询压力。
        2. 分治。在软件设计中，有一种分治的思想，对于存储瓶颈的问题，业界常用的方案就是分而治之：流量分散、存储分散，即：分库分表。
        3. 发券，归根结底是要对用户的领券记录做持久化存储。对于 MySQL 本身 I/O瓶颈来说，我们可以在不同服务器上部署 MySQL 的不同分片，对 MySQL做水平扩容，这样一来，写请求就会分布在不同的 MySQL 主机上，这样就能够大幅提升 MySQL 整体的吞吐量。
        4. 给用户发了券，那么用户肯定需要查询自己获得的券。基于这个逻辑，我们以user_id 后四位为分片键，对用户领取的记录表做水平拆分，以支持用户维度的领券记录的查询。
        5. 每种券都有对应的数量，在给用户发券的过程中，我们是将发券数记录在Redis 中的，大流量的情况下，我们也需要对 Redis 做水平扩容，减轻 Redis单机的压力。

# 14. 如何避免超预期的高并发压力压垮系统？
    解决这种问题的一个主要手段就是限流，即拒绝部分访问请求，使访问负载压力降低到一个系统可以承受的程度。这样虽然有部分用户访问失败，但是整个系统依然是可用的，依然能对外提供服务，而不是因为负载压力太大而崩溃，导致所有用户都不能访问。为此，我们准备开发一个限流器，产品名称为“Diana”。
## 14.1 需求分析
    我们将 Diana 定位为一个限流器组件，即 Diana 的主要应用场景是部署在微服务网关或者其他 HTTP 服务器入口，以过滤器的方式对请求进行过滤，对超过限流规则的请求返回“服务不可用”HTTP 响应。
    Diana 的限流规则可通过配置文件获取，并需要支持本地配置和远程配置两种方式，远程配置优先于本地配置。限流方式包括：
         全局限流：针对所有请求进行限流，即保证整个系统处理的请求总数满足限流配置。
         账号限流：针对账号进行限流，即对单个账号发送的请求进行限流。
         设备限流：针对设备进行限流，即对单个客户端设备发送的请求进行限流。
         资源限流：针对某个资源（即某个 URL）进行限流，即保证访问该资源的请求总数满足限流配置。
    并且 Diana 设计应遵循开闭原则，能够支持灵活的限流规则功能扩展，即未来在不修改现有代码和兼容现有配置文件的情况下，支持新的配置规则。
### 14.2.2 高可用设计
   为了保证配置中心服务器和 Redis 服务器宕机时，限流器组件的高可用。限流器应具有自动降级功能，即配置中心不可用，则使用本地配置；Redis 服务器不可用，则降级为本地限流。
## 14.3 详细设计
    常用的限流算法有 4 种，
    固定窗口（Window）限流算法，滑动窗口（Sliding Window）限流算法，漏桶（Leaky Bucket）限流算法，令牌桶（Token Bucket）限流算法。

# 17. 如何让系统抗住双十一的预约抢购活动？
    我们先把需求梳理一下，总的来说，实现一个抢购系统大概可以分为四个阶段。
     商品预约：用户进入商品详情页面，获取购买资格，并等待商品抢购倒计时。
     等待抢购：等待商品抢购倒计时，直到商品开放抢购。
     商品抢购：商品抢购倒计时结束，用户提交抢购订单，排队等待抢购结果，抢购成功后，扣减系统库存，生成抢购订单。
     订单支付：等待用户支付成功后，系统更新订单状态，通知用户购买成功。
    接下来，我们就针对各阶段容易出现的问题，来分析其中的技术考点和解决方案
# 17.1 商品预约阶段
    那么问题来了：如何在高并发量的情况下，让每个用户都能得到抢购资格呢？这是预约抢购场景第一个技术考察点。 那你可以基于“06 | 分布式系统中，如何回答锁的实现原理？”来控制抢购资格的发放。
    我们基于 Redis 实现分布式锁（这是最常用的方式），在加锁的过程中，实际上是给Key 键设置一个值，为避免死锁，还要给
        SET lock_key unique_value NX PX 1000
     lock_key 就是 key 键；
     unique_value 是客户端生成的唯一的标识；
     NX 代表只在 lock_key 不存在时，才对 lock_key 进行设置操作；
     PX 10000 表示设置 lock_key 的过期时间为 10s，这是为了避免客户端发生异常而无法释放锁。
# 17.2 等待抢购阶段
    那么问题来了：如何解决等待抢购时间内的流量突增问题呢？有两个解决思路。
     页面静态化
     服务端限流
# 17.3 商品抢购阶段
    从“商品抢购阶段的架构设计”中我们可以总结出三个技术考点：流量削峰、扣减库存、分库分表。
# 17.4 订单支付阶段
    订单支付后操作（异常）
    所以你还要考虑“05 | 海量并发场景下，如何回答分布式事务一致性问题？”中，可靠消息投递机制：
    先做消息的本地存储，再通过异步重试机制，来实现消息的补偿。
    比如当支付平台回调订单系统，然后在更新状态的同时，插入一个消息，之后再返回第三方支付操作成功的结果。
    最后，通过数据库中的这条消息，再异步推送其他系统，完成后续的工作。
    mjj个人答疑解惑：(详见 SpringBoot订单支付后消息可靠补偿方案.md)
        SpringBoot订单支付后消息可靠补偿方案
        在订单支付后的操作中，发送异步消息到积分和短信服务时可能会遇到网络抖动、服务不可用等异常情况。下面提供一个完整的可靠消息补偿方案，包含本地消息表和异步重试机制。
            方案设计
            1. 本地消息表：在业务数据库中创建消息表，记录需要发送的消息
            2. 事务一致性：将业务操作和消息记录放在同一个事务中
            3. 定时任务：定期扫描未完成的消息进行重试
            4. 幂等处理：消费端需要实现幂等性，防止重复消费

# 18. 如果让你设计一个支持千万级流量的架构，你会怎么设计？
## 18.1 前端优化
    前端的优化主要有三个环节：减少请求次数、页面静态化、边缘计算。
        减少请求次数：减少前端脚本与后端服务的请求次数，有三种方案。
         增加缓存控制
         减少图像的请求次
         减少脚本的请求
## 18.2 后端优化
    后端环节的性能问题，可以从基础设施层面、网络层面、架构层面三个角度进行考量

# 19. 如何设计 RPC 框架在 10 万 QPS 下实现毫秒级的服务调用？
## 19.2 如何提升网络传输性能
    常见的五种 I/O 模型：
        同步阻塞 I/O；
        同步非阻塞 I/O；
        同步多路 I/O 复用；
        信号驱动 I/O；
        异步 I/O。
    你需要理解这五种 I/O 模型的区别和特点，不过在理解上你可能会有些难度，所以我来做个比喻方便你理解。
    我们把 I/O 过程比喻成烧水倒水的过程，等待资源（就是烧水的过程），使用资源（就是倒水的过程）：
        如果你站在灶台边上一直等着（等待资源）水烧开，然后倒水（使用资源），那么就是同步阻塞 I/O；
        如果你偷点儿懒，在烧水的时候躺在沙发上看会儿电视（不再时时刻刻等待资源），但是还是要时不时地去看看水开了没有，一旦水开了，马上去倒水（使用资源），那么这就是同步非阻塞 I/O；
        如果你想要洗澡，需要同时烧好多壶水，那你就在看电视的间隙去看看哪壶水开了（等待多个资源），哪一壶开了就先倒哪一壶，这样就加快了烧水的速度，这就是同步多路I/O 复用；
        不过你发现自己总是跑厨房去看水开了没，太累了，于是你考虑给你的水壶加一个报警器（信号），只要水开了就马上去倒水，这就是信号驱动 I/O；
        最后一种就高级了，你发明了一个智能水壶，在水烧好后自动就可以把水倒好，这就是异步 I/O。
    这五种 I/O 模型中最被广泛使用的是多路 I/O 复用，Linux 系统中的 select、epoll等系统调用都是支持多路 I/O 复用模型的，Java 中的高性能网络框架 Netty 默认也是使用这种模型。你可以选择它。
## 19.3 选择合适的序列化方式
    综合上面的几个考虑点，在我看来，我们的序列化备选方案主要有以下几种：
        首先是大家熟知的 JSON，它起源于 JavaScript 是一种最广泛使用的序列化协议，它的优势简单易用，同时在性能上相比 XML
        Thrift 是 Facebook 开源的高性能的序列化协议，也是一个轻量级的 RPC 框架；
        Protobuf 是谷歌开源的序列化协议。它们的共同特点是无论在空间上还是时间上都有着很高的性能，缺点就是由于 IDL 存在带来一些使用上的不方便
    那么你要如何选择这几种序列化协议呢？这里我给你几点建议：
    如果对于性能要求不高，在传输数据占用带宽不大的场景下可以使用 JSON 作为序列化协议；
    如果对于性能要求比较高，那么使用 Thrift 或者 Protobuf 都可以。而 Thrift 提供了配套的 RPC 框架，所以想要一体化的解决方案，你可以优先考虑 Thrift；
    在一些存储的场景下，比如说你的缓存中存储的数据占用空间较大，那么你可以考虑使用 Protobuf 替换 JSON

# 21. 如何根据应用场景选择合适的消息中间件？
    引入消息中间件的目的是让它来扛住海量流量，流量先进入到消息队列中，然后消费端下游系统可以慢慢消费消息中间件中的数据，这样能有效保护下游系统不被瞬时的流量击破。

    在进行具体的选型时，我们可以结合自己团队的实际情况。
     如果公司或团队的技术栈以 Golang 为主，建议选择 RabbitMQ，RabbitMQ 在性能上的缺陷可以通过搭建多套集群加以规避。
     如果公司或团队的技术栈以 Java 为主，我建议使用 Kafka 或 RocketMQ。RocketMQ 和 Kafka 都是性能优秀的中间件，在这两者之间进行选择时可以更多地关注功能特性。RocketMQ 提供了消息重试、消息过滤、消息轨迹、消息检索
        等功能特性，特别是 RocketMQ 的消息检索功能，因此 RocketMQ 很适合核心业务场景。而 kafka 更加擅长于日志、大数据计算、流式计算等场景。

# 22. 如何提升 RocketMQ 顺序消费性能
## 22.1 RocketMQ 顺序消费设计缺
    回顾上面 RocketMQ 实现顺序消费的核心关键词，我们发现其实就是加锁、加锁、加锁。没错，为了实现顺序消费，RocketMQ 需要进行三次加锁：
     进行队列负载平衡后，对新分配的队列，并不能立即进行消息拉取，必须先在Broker 端获取队列的锁；
     消费端在正式消费数据之前，需要锁定 MessageQueue 和 ProceeQueue。上述三把锁的控制，让并发度受到了队列数量的限制。在互联网、高并发编程领域，通常是“谈锁色变”，锁几乎成为了性能低下的代名词。试图减少锁的使用、缩小锁的范围几乎是性能优化的主要手段。

# 23. 使用分布式调度框架该考虑哪些问题？
    因为定时调度任务的调度频率直接决定了消息发送的实时性，随着需要调度的任务越来越多，大部分定时调度框架对秒级别的定时调度都不太友好。这时的调度通常都是分钟级的，但分钟级的调度会给任务带来较大的延迟，这是大部分业务无法容忍的，怎么办呢？

    ElasticJob 可以通过支持流式任务解决这个问题。具体的思路是：将任务配置为按照分钟级进行调度，例如每分钟执行一次调度。每次调度按照分页去查找数据，处理完一批数据，再查询下一批，如果查到待处理数据，就继续处理数据，直到没有待处理数据时，
    才结束本次业务处理。如果本次处理时间超过了一个调度周期，那么利用 ElasticJob 的任务错过补偿执行机制会再触发一次调度。

    在业务高峰期，这种方式基本上提供了准实时的处理效果。只有在业务量较少时，如果处理完一批数据后没有其他待处理的数据，这时新到的数据才会延迟 1 分钟执行。
    综合来看，通过支持流式任务，我们可以极大地提高数据的处理时效。

    具体实现看 个人工作总结 2025/05/13 - java ElasticJob支持流式任务

# 26. 如何解决高并发下的库存抢购超卖少买问题？
## 26.3 令牌库存
    具体是使用 Redis 中的 list 保存多张令牌来代表库存，一张令牌就是一个库存，用户抢库存时拿到令牌的用户可以继续支付
        1 //放入三个库存
        2 redis> lpush prod_1475_stock_queue_1 stock_1
        3 redis> lpush prod_1475_stock_queue_1 stock_2
        4 redis> lpush prod_1475_stock_queue_1 stock_3
        5
        6 //取出一个，超过 0.5 秒没有返回，那么抢库存失败
        7 redis> brpop prod_1475_stock_queue_1 0.5
    具体实现看 个人工作总结 2025/05/13 - 使用RedisTemplate的List实现库存令牌管理
## 26.4 多库存秒杀
## 26.5 自旋互斥超时锁
## 26.6 CAS 乐观锁：锁操作后置
    除此之外我再推荐一个实现方式：CAS 乐观锁。相对于自旋互斥锁来说，它在并发争抢库存线程少的时候效率会更好。通常，我们用锁的实现方式是先抢锁，然后，再对数据进行操作。这个方式需要先抢到锁才能继续，而抢锁是有性能损耗的，即使没有其他线程抢锁，这个消耗仍旧存在。
    CAS 乐观锁的核心实现为：记录或监控当前库存信息或版本号，对数据进行预操作。
## 26.7 Redis Lua 方式实现 Redis 锁
    与“事务+乐观锁”类似的实现方式还有一种，就是使用 Redis 的 Lua 脚本实现多步骤库存操作。因为 Lua 脚本内所有操作都是连续的，这个操作不会被其他操作打断，所以不存在锁争抢问题。

# 27. 为什么高并发下数据写入不推荐关系数据库？
    总结(deepseek)
    高并发下不推荐关系数据库的主要原因在于其索引结构、锁机制、ACID特性实现和主从架构等方面的设计限制了写入性能。B+Tree索引的随机IO、锁竞争、事务开销和单点写入等问题使得关系数据库难以应对极高并发的写入场景1310。
    对于电商供应链系统，没有放之四海而皆准的数据库解决方案。合理的策略是根据不同业务场景选择最适合的数据库技术，形成混合架构：
        核心事务：关系数据库（MySQL/PostgreSQL）+ Redis缓存
        商品信息：文档数据库（MongoDB）
        用户行为：搜索引擎（ElasticSearch）或时序数据库
        分析报表：列式存储（ClickHouse）或数据仓库
        推荐系统：图数据库（Neo4j）

# 31. 假设数据库成为了性能瓶颈点，动态数据查询如何提升效率
## 31.4 缓存分类
   在我们日常开发中，常见的缓存主要就是静态缓存、分布式缓存和热点本地缓存这三种。
       静态缓存在 Web 1.0 时期是非常著名的，它一般通过生成 Velocity 模板或者静态HTML 文件来实现静态缓存。
       分布式缓存的大名可谓是如雷贯耳了，我们平时耳熟能详的 Memcached、Redis 就是分布式缓存的典型例子。
       那么我们会在代码中使用一些本地缓存方案，如 HashMap，Guava Cache 或者是Ehcache 等，
## 31.5 缓存的不足
    首先，缓存比较适合于读多写少的业务场景，并且数据最好带有一定的热点属性。这是因为缓存毕竟会受限于存储介质不可能缓存所有数据，那么当数据有热点属性的时候才能保证一定的缓存命中率。
    其次，缓存会给整体系统带来复杂度，并且会有数据不一致的风险。当更新数据库成功，更新缓存失败的场景下，缓存中就会存在脏数据。
    再次，之前提到缓存通常使用内存作为存储介质，但是内存并不是无限的。因此，我们在使用缓存的时候要做数据存储量级的评估，对于可预见的需要消耗极大存储成本的数据，要慎用缓存方案。同时，缓存一定要设置过期时间，这样可以保证缓存中的会是热点数据。
    最后，缓存会给运维也带来一定的成本，运维需要对缓存组件有一定的了解，在排查问题的时候也多了一个组件需要考虑在内。

# 34. 线上的 API 接口响应比较慢，该如何快速排查和定位问题？
## 34.1 定位问题
### 34.1.1 慢查询日志
    通常情况下，为了定位 sql 的性能瓶颈，我们需要开启 mysql 的慢查询日志。把超过指定时间的 sql 语句，单独记录下来，方面以后分析和定位问题。
    开启慢查询日志需要重点关注三个参数：
         slow_query_log 慢查询开关
         slow_query_log_file 慢查询日志存放的路径
         long_query_time 超过多少秒才会记录日志
    通过 mysql 的 set 命令可以设置：
        1 set global slow_query_log='ON';
        2 set global slow_query_log_file='/usr/local/mysql/data/slow.log';
        3 set global long_query_time=2;
    设置完之后，如果某条 sql 的执行时间超过了 2 秒，会被自动记录到 slow.log 文件中。
    当然也可以直接修改配置文件 my.cnf
        1 [mysqld]
        2 slow_query_log = ON
        3 slow_query_log_file = /usr/local/mysql/data/slow.log
        4 long_query_time = 2
    但这种方式需要重启 mysql服务
### 34.1.2 监控
     Metrics - 监控指标
        系统性能指标，包括请求成功率、系统吞吐量、响应时长
        资源性能指标，衡量系统软硬件资源使用情况，配合系统性能指标，观察系统资源水位
     Logs - 日志
        施压引擎日志，观察施压引擎是否健康，压测脚本执行是否有报错
        采样日志，采样记录 API 的请求和响应详情，辅助排查压测过程中的一些出错请求的参数是否正常，并通过响应详情，查看完整的错误信息
     Traces - 分布式链路追踪
        用于性能问题诊断阶段，通过追踪请求在系统中的调用链路，定位报错 API 的报错系统和报错堆栈，快速定位性能问题点。
## 34.3 链路追踪
    有时候某个接口涉及的逻辑很多，比如：查数据库、查 redis、远程调用接口，发 mq消息，执行业务代码等等。
    该接口一次请求的链路很长，如果逐一排查，需要花费大量的时间，这时候，我们已经没法用传统的办法定位问题了。有没有办法解决这问题呢？
    用分布式链路跟踪系统：skywalking。通过 skywalking 定位性能问题
    在 skywalking 中可以通过 traceId（全局唯一的 id），串联一个接口请求的完整链路。
    可以看到整个接口的耗时，调用的远程服务的耗时，访问数据库或者 redis 的耗时等等，功能非常强大。
    之前没有这个功能的时候，为了定位线上接口性能问题，我们还需要在代码中加日志，手动打印出链路中各个环节的耗时情况，然后再逐一排查。
## 34.4 问题排查
### 34.4.1 个别接口响应慢
#### 34.4.1.1 链路追踪
    使用 SkyWalking，展示出每一个与网络有关的耗时。比如：读写数据库、读写 Redis、
    SpringCloud 调用、Dubbo 调用等。这样就能立马定位是哪次操作耗时了。
    同时，SkyWalking 可以记录每一个 SQL 语句，可以帮助定位
## 34.5 垃圾回收
### 34.5.1 什么情况下，GC 会对程序产生影响？
    不管 Minor GC 还是 FGC，都会造成一定程度的程序卡顿（即 Stop The World：GC线程开始工作，其他工作线程被挂起），即使采用 ParNew、CMS 或者 G1 这些更先进的垃圾回收算法，也只是在减少卡顿时间，而并不能完全消除卡顿。
    那到底什么情况下，GC 会对程序产生影响呢？根据严重程度从高到底，包括以下 4 种情况：
    1. FGC 过于频繁
        a. FGC 通常比较慢，少则几百毫秒，多则几秒，正常情况 FGC 每隔几个小时甚至几天才执行一次，对系统的影响还能接受。
        b. 一旦出现 FGC 频繁（比如几十分钟执行一次），是存在问题的，会导致工作线程频繁被停止，让系统看起来一直有卡顿现象，也会使得程序的整体性能变差。
    2. YGC 耗时过长
        执行频率高：通常几秒到几分钟就会发生一次
        a. 一般来说，YGC 的总耗时在几十或者上百毫秒是比较正常的，虽然会引起系统卡顿几毫秒或者几十毫秒，这种情况几乎对用户无感知，对程序的影响可以忽略不计。
        b. 如果 YGC 耗时达到了 1 秒甚至几秒（都快赶上 FGC 的耗时了），那卡顿时间就会增大，加上 YGC 本身比较频繁，就会导致比较多的服务超时问题。
    3. FGC 耗时过长
        a. FGC 耗时增加，卡顿时间也会随之增加，尤其对于高并发服务，可能导致 FGC期间比较多的超时问题，可用性降低，这种也需要关注。
    4. YGC 过于频繁
        a. 即使 YGC 不会引起服务超时，但是 YGC 过于频繁也会降低服务的整体性能，对于高并发服务也是需要关注的。
### 34.5.2 如何排查
    1. 公司的监控系统：大部分公司都会有，可全方位监控 JVM 的各项指标。
    2. JDK 自带工具：jmap、jstat
        a. 查看堆内存各区域的使用率以及 GC 情况：jstat -gcutil pid 1000 （重点关注结果中的 YGC、YGCT、FGC、FGCT、GCT）
        b. 查看堆内存中的存活对象，并按空间排序：jmap -histo pid | head -n20
        c. dump 堆内存文件：jmap -dump:format=b,file=heap pid

# 35. 百万级别数据的 Excel 如何快速导入到数据库中
    详见https://github.com/chasesunshine/mjj-test-new-technology/demo-mybatis-plus11/doc/Spring Boot 多线程导入百万级Excel数据到数据库.md
## 35.6 总结
    提升 Excel 导入速度的方法：
    1.使用更快的 Excel 读取框架(推荐使用阿里 EasyExcel)
    2.对于需要与数据库交互的校验、按照业务逻辑适当的使用缓存。用空间换时间
    3.使用 values(),(),() 拼接长 SQL 一次插入多行数据
    4.使用多线程插入数据，利用掉网络 IO 等待时间(推荐使用并行流，简单易用)
    5.避免在循环中打印无用的日志



# 目前看到51. [阿里一面]购物车系统怎么设计？