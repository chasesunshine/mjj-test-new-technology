# 2. 电商平台中订单未支付过期如何实现自动关单？
## 定时任务 (个人不建议)
优点：
    实现容易，成本低，基本不依赖其他组件。
缺点：
    时间可能不够精确。由于定时任务扫描的间隔是固定的，所以可能造成一些订单已经过期了一段时间才被扫描到，订单关闭的时间比正常时间晚一些。
    增加了数据库的压力。随着订单的数量越来越多，扫描的成本也会越来越大，执行时间也会被拉长，可能导致某些应该被关闭的订单迟迟没有被关闭。
总结：
    采用定时任务的方案比较适合对时间要求不是很敏感，并且数据量不太多的业务场景。
## JDK 延迟队列 DelayQueue
    DelayQueue 是 JDK 提供的一个无界队列，我们可以看到，DelayQueue 队列中的元素需要实现 Delayed，它只提供了一个方法，就是获取过期时间
    用户的订单生成以后，设置过期时间比如 30 分钟，放入定义好的 DelayQueue，然后创建一个线程，在线程中通过 while(true)不断的从 DelayQueue 中获取过期的数据。
    优点：
        不依赖任何第三方组件，连数据库也不需要了，实现起来也方便。
    缺点：
        因为 DelayQueue 是一个无界队列，如果放入的订单过多，会造成 JVMOOM。DelayQueue 基于 JVM 内存，如果 JVM 重启了，那所有数据就丢失了。
    总结：
        DelayQueue 适用于数据量较小，且丢失也不影响主业务的场景，比如内部系统的一些非重要通知，就算丢失，也不会有太大影响
个人解惑:
    while(true)不断的从 DelayQueue 中获取过期的数据。用的是@PostConstruct之类的方案做处理
## redis 过期监听
    redis 是一个高性能的 KV 数据库，除了用作缓存以外，其实还提供了过期监听的功能。在 redis.conf 中，配置 notify-keyspace-events Ex 即可开启此功能。
    然后在代码中继承 KeyspaceEventMessageListener，实现 onMessage 就可以监听过期的数据量。
    优点：
        由于redis的高性能，所以我们在设置key，或者消费key时，速度上是可以保证的。
    缺点：
        由于redis的key过期策略原因，当一个key过期时，redis无法保证立刻将其删除，自然我们的监听事件也无法第一时间消费到这个key，所以会存在一定的延迟。
        另外，在redis5.0之前，订阅发布中的消息并没有被持久化，自然也没有所谓的确认机制。所以一旦消费消息的过程中我们的客户端发生了宕机，这条消息就彻底丢失了。
    总结：
        redis 的过期订阅相比于其他方案没有太大的优势，在实际生产环境中，用得相对较少。
## Redisson 分布式延迟队列
    Redisson 是一个基于 redis 实现的 Java 驻内存数据网格，它不仅提供了一系列的分布式的 Java 常用对象，还提供了许多分布式服务。
    Redisson 除了提供我们常用的分布式锁外，还提供了一个分布式延迟队列RDelayedQueue，他是一种基于 zset 结构实现的延迟队列，其实现类是RedissonDelayedQueue。
    优点：
        使用简单，并且其实现类中大量使用 lua 脚本保证其原子性，不会有并发重复问题。
    缺点：
        需要依赖 redis（如果这算一种缺点的话）。
    总结：
        Redisson 是 redis 官方推荐的 JAVA 客户端，提供了很多常用的功能，使用简单、高效，推荐大家尝试使用
## RocketMQ 延迟消息
    延迟消息，当消息写入到 Broker 后，不会立刻被消费者消费，需要等待指定的时长后才可被消费处理的消息，称为延时消息。
    在订单创建之后，我们就可以把订单作为一条消息投递到 rocketmq，并将延迟时间设置为 30 分钟，这样，30 分钟后我们定义的 consumer 就可以消费到这条消息，然后检查用户是否支付了这个订单
    优点：
        可以使代码逻辑清晰，系统之间完全解耦，只需关注生产及消费消息即可。另外其吞吐量极高，最多可以支撑万亿级的数据量。
    缺点：
        相对来说 mq 是重量级的组件，引入 mq 之后，随之而来的消息丢失、幂等性问题等都加深了系统的复杂度。
    总结：
        通过 mq 进行系统业务解耦，以及对系统性能削峰填谷已经是当前高性能系统的标配。
## RabbitMQ 死信队列
    除了 RocketMQ 的延迟队列，RabbitMQ 的死信队列也可以实现消息延迟功能。当 RabbitMQ 中的一条正常消息，因为过了存活时间（TTL 过期）、队列长度超限、被消费者拒绝等原因无法被消费时，就会被当成一条死信消息，投递到死信队列。
    基于这样的机制，我们可以给消息设置一个 ttl，然后故意不消费消息，等消息过期就会进入死信队列，我们再消费死信队列即可。通过这样的方式，就可以达到同 RocketMQ 延迟消息一样的效果。
    优点：
        同 RocketMQ 一样，RabbitMQ 同样可以使业务解耦，基于其集群的扩展性，也可以实现高可用、高性能的目标。
    缺点：
        死信队列本质还是一个队列，队列都是先进先出，如果队头的消息过期时间比较长，就会导致后面过期的消息无法得到及时消费，造成消息阻塞。
    总结：
        除了增加系统复杂度之外，死信队列的阻塞问题也是需要我们重点关注的。

# 3. 如何设计一个秒杀系统
    在我看来，秒杀系统本质上就是一个满足大并发、高性能和高可用的分布式系统
    在我看来，秒杀其实主要解决两个问题，一个是并发读，一个是并发写。
## 3.3.4 处理热点数据
    处理热点数据通常有几种思路：一是优化，二是限制，三是隔离。
    先来说说优化。优化热点数据最有效的办法就是缓存热点数据，如果热点数据做了动静分离，那么可以长期缓存静态数据。但是，缓存热点数据更多的是“临时”缓存，即不管是静态数据还是动态数据，都用一个队列短暂地缓存数秒钟，由于队列长度有限，可以采用 LRU 淘汰算法替换
    再来说说限制。限制更多的是一种保护机制，限制的办法也有很多，例如对被访问商品的 ID 做一致性 Hash，然后根据 Hash 做分桶，每个分桶设置一个处理队列，这样可以把热点商品限制在一个请求队列里，防止因某些热点商品占用太多的服务器资源，而使其他请求始终得不到服务器的处理资源。
    最后介绍一下隔离。秒杀系统设计的第一个原则就是将这种热点数据隔离出来，不要让1%的请求影响到另外的 99%，隔离出来后也更方便对这 1%的请求做针对性的优化。具体到“秒杀”业务，我们可以在以下几个层次实现隔离。
        1. 业务隔离。把秒杀做成一种营销活动，卖家要参加秒杀这种营销活动需要单独报名，从技术上来说，卖家报名后对我们来说就有了已知热点，因此可以提前做好预热。
        2. 系统隔离。系统隔离更多的是运行时的隔离，可以通过分组部署的方式和另外 99%分开。秒杀可以申请单独的域名，目的也是让请求落到不同的集群中。
        3. 数据隔离。秒杀所调用的数据大部分都是热点数据，比如会启用单独的 Cache 集群或者 MySQL 数据库来放热点数据，目的也是不想 0.01%的数据有机会影响99.99%数据。

# 4. 如果你的系统的 QPS 突然提升 10 倍你会怎么设计？
## 4.1 硬件的扩展+微服务的拆分
## 4.2 高性能 RPC
    有小伙伴进行对比测试，Dubbo RPC 的性能，是 Feign RPC 的性能 10 倍。(这一点 mjj 本人提出质疑)
    mjj个人搜索解答：
        Dubbo RPC 在性能上优于 Feign RPC，主要原因包括以下几个方面：
        1. **协议与传输层优化**
           - **Dubbo** 默认采用 **Dubbo 协议**，基于 **TCP 长连接** 和 **Netty** 进行通信，减少了连接建立和断开的开销，适合高并发场景。
           - **Feign** 基于 **HTTP/1.1**（短连接），每次请求都需要建立和断开 TCP 连接，增加了网络开销，性能较低。
        2. **序列化方式**
           - **Dubbo** 支持 **Hessian2、Protobuf** 等高效的二进制序列化方式，数据体积小，解析速度快。
           - **Feign** 默认使用 **JSON/XML** 等文本格式，数据体积较大，解析效率较低。
        3. **通信模型**
           - **Dubbo** 支持 **异步调用** 和 **NIO（Netty）**，能更好地利用系统资源，提高吞吐量。
           - **Feign** 基于 **同步阻塞式 HTTP 调用**，在高并发场景下容易出现性能瓶颈。
        4. **负载均衡与容错机制**
           - **Dubbo** 内置 **多种负载均衡策略**（随机、轮询、最少活跃调用等），并支持 **服务端和客户端负载均衡**，减少网络延迟。
           - **Feign** 依赖 **Ribbon（或 Spring Cloud LoadBalancer）**，仅支持客户端负载均衡，策略相对较少。
        5. **服务治理优化**
           - **Dubbo** 提供 **更细粒度的流量控制**（如并发控制、超时管理），减少无效请求，提高整体性能。
           - **Feign** 的容错依赖 **Hystrix（已停止维护）** 或 **Sentinel**，相比 Dubbo 的 **Failover/Failfast 等策略**，性能优化空间较小。
        6. **网络层优化**
           - **Dubbo** 的 **RPC 协议** 属于 **传输层**，比 **Feign（应用层 HTTP）** 减少了协议解析的开销。
           - **Dubbo 支持多协议**（如 gRPC、RMI），可根据业务需求选择最优方案。
        **总结**
        Dubbo 在 **协议优化、序列化效率、通信模型、负载均衡、服务治理** 等方面均优于 Feign，因此在高并发、低延迟场景下性能更高。而 Feign 的优势在于 **易用性、RESTful 兼容性**，适合中小规模微服务架构。
## 4.3 消息队列消峰解耦
## 4.4 三级缓存架构
    以秒杀系统举例，活动预热商品信息可以提前缓存提供查询服务，库存数据可以提前缓存，下单流程可以完全走缓存扣减，秒杀结束后再异步写入数据库，数据库承担的压力就小的太多了。
## 4.5 数据库分库分表
    读写分离也就相当于数据库集群的方式降低了单节点的压力。而面对数据的急剧增长，原来的单库单表的存储方式已经无法支撑整个业务的发展，这时候就需要对数据库进行分库分表了。针对微服务而言垂直的分库本身已经是做过的，剩下大部分都是分表的方案了。
## 4.6 高可用
## 4.7 总结
   其实可以看到，怎么设计高并发系统这个问题本身他是不难的，无非是基于你知道的知识点，从物理硬件层面到软件的架构、代码层面的优化，使用什么中间件来不断提高系统的抗压能力。
   但是这个问题本身会带来更多的问题，微服务本身的拆分带来了分布式事务的问题，http、RPC 框架的使用带来了通信效率、路由、容错的问题，MQ 的引入带来了消息丢失、积压、事务消息、顺序消息的问题，
   缓存的引入又会带来一致性、雪崩、击穿的问题，数据库的读写分离、分库分表又会带来主从同步延迟、分布式 ID、事务一致性的问题，而为了解决这些问题我们又要不断的加入各种措施熔断、限流、降级、离线核对、预案处理等等来防止和追溯这些问题。



# 目前看到51. [阿里一面]购物车系统怎么设计？